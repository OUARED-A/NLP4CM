CliffGuard: A Principled Framework for Finding Robust
Database Designs
To fulfill,this,crucial,"role,",a,database,(or,its,administrator),must
make many important decisions on how to provision and tune the,,,,,,,,,
"system in order to deliver the best performance possible,",,,,,,,,such as,
which materialized,,"views,",,"indices,",or,samples,,to build. While,
these auxiliary structures can significantly,,,,,,,"improve performance,",,
they also incur storage and maintenance overheads.,,,,,,,,"In fact, most",
practical budgets only allow for building a handful of indices and a,,,,,,,,,
dozen materialized views out of an exponential number of possible,,,,,,,,,
structures.,"For instance, for a data-warehouse with 100 columns,",,,,,,,,
there are at least Ω(3100) sorted projections to choose from (each,,,,,,,,,
"column can be either absent,",,,,,or in ascending/ descending order).,,,,
"Thus, a fundamental database problem is finding the best physical",,,,,,,,,
design; that,"is, finding a set of indices and/or materialized views",,,,,,,,
that optimizes the performance of future queries.,,,,,,,,,
Modern databases come with designer tools (a.k.a.,,,,,,,,auto-tuning,
"tools) that take certain parameters of a target workload (e.g., queries,",,,,,,,,,
"data distribution, and various cost estimates) as input, and then use",,,,,,,,,
different heuristics to search the design space and find an optimal,,,,,,,,,
"design (e.g.,",a,set of,indices,,or materialized,,,views) within,their
"time and storage budgets. However, these designs are only optimal",,,,,,,,,
for the input parameters provided to the designer.,,,,,,,,"Unfortunately,",
"in practice, these parameters are subject to many sources of uncer-",,,,,,,,,
"tainty, such as noisy environments, approximation errors (e.g.,",,,,,,,,,in
"the query optimizer’s cost or cardinality estimates [11]), and miss-",,,,,,,,,
"ing or time-varying parameters. Most notably, since future queries",,,,,,,,,
"are unknown, these tools usually optimize for past queries in hopes",,,,,,,,,
that future ones will be similar.,,,,,,,,,
"Existing designer tools (e.g., Index Tuning Wizard [7] and Tun-",,,,,,,,,
"ing Advisor in Microsoft SQL Server [26], Teradata’s Index Wizard",,,,,,,,,
"[20], IBM DB2’s Design Advisor [76], Oracle’s SQL Tuning Ad-",,,,,,,,,
"viser [31], Vertica’s DBD [47, 71], and Parinda for Postgres [54])",,,,,,,,,
do not take into account the influence of such uncertainties on the,,,,,,,,,
"optimality of their design, and therefore, produce designs that are",,,,,,,,,
sub-optimal and remarkably brittle.,,,,,We call all these existing de-,,,,
signers nominal.,,That,"is,",all,these tools assume,,,that their,input
parameters,are,precisely,known,,and equal,,to,some nominal,val-
"ues. As a result,",,overall performance often plummets as soon as,,,,,,,
"future workload deviates from the past (say, due to the arrival of",,,,,,,,,
new data or a shift in day-to-day queries).,,,,,,,These dramatic perfor-,,
mance decays are severely disruptive for time-critical applications.,,,,,,,,,
They also,waste,critical,human,,and computational,,,"resources,",as
dissatisfied customers,,,request vendor,,,"inspections,",,often resulting,
in re-tuning/re-designing the database to restore the required level,,,,,,,,,
of performance.,,,,,,,,,
"Our Goal — To overcome the shortcomings of nominal designers,",,,,,,,,,
we propose a new type of designers that are immune to parame-,,,,,,,,,
ter uncertainties as much as desired;,,,,,"that is, they are robust.",,,,Our
robust designer gives database administrators a knob to decide ex-,,,,,,,,,
A fundamental problem in database systems is choosing the best
"physical design, i.e., a small set of auxiliary structures that en-"
able the fastest execution of future queries. Almost all commer-
cial databases come with designer tools that create a number of
indices or materialized views (together comprising the physical de-
sign) that they exploit during query processing. Existing designers
"are what we call nominal; that is, they assume that their input pa-"
rameters are precisely known and equal to some nominal values.
"For instance, since future workload is often not known a priori, it"
is common for these tools to optimize for past workloads in hopes
"that future queries and data will be similar. In practice, however,"
these parameters are often noisy or missing. Since nominal design-
"ers do not take the influence of such uncertainties into account, they"
"find designs that are sub-optimal and remarkably brittle. Often, as"
"soon as the future workload deviates from the past, their overall per-"
"formance falls off a cliff. Thus, we propose a new type of database"
"designer that is robust against parameter uncertainties, so that over-"
all performance degrades more gracefully when future workloads
deviate from the past. Users express their risk tolerance by decid-
ing on how much nominal optimality they are willing to trade for
attaining their desired level of robustness against uncertain situa-
"tions. To the best of our knowledge, this paper is the first to adopt"
the recent breakthroughs in robust optimization theory to build a
practical framework for solving one of the most fundamental prob-
"lems in databases, replacing today’s brittle designs with robust de-"
signs that guarantee a predictable and consistent performance.
Categories and Subject Descriptors
H.2.2 [Database Management]: [Physical Design]
Physical Design; Robust Optimization; Workload Resilience
1. INTRODUCTION
Database management systems are among the most critical soft-
ware components in our world today. Many important applications
"across enterprise, science, and government depend on database tech-"
nology to derive insight from their data and make timely decisions.
actly how much nominal optimality to trade for a desired level of
"robustness. For instance, users may demand a set of optimal materi-"
alized views with an assurance that they must remain robust against
change in their workload of up to 30%. A more conservative user
"may demand a higher degree of robustness, say 60%, at the expense"
of less nominal optimality. Robust designs are highly superior to
"nominal ones, as:"
"(a) Nominal designs are inherently brittle and subject to perfor-"
"mance cliffs, while the performance of a robust design will de-"
grade more gracefully.
"(b) By taking uncertainties into account, robust designs can guard"
"against worst-case scenarios, delivering a more consistent and"
predictable performance to time-sensitive applications.
"(c) Given the highly non-linear and complex (and possibly non-"
"convex) nature of database systems, a workload may have more"
"than one optimal design. Thus, it is completely conceivable"
"that a robust design may be nominally optimal as well (see [15,"
16] for such examples in other domains).
"(d) A robust design can significantly reduce operational costs by"
requiring less frequent database re-designs.
Previous Approaches — There has been some pioneering work on
"incorporating parameter uncertainties in databases [11, 25, 30, 35,"
"56, 63]. These techniques are specific to run-time query optimiza-"
tion and do not easily extend to physical designs. Other heuristics
have been proposed for improving physical designs through work-
"load compression (i.e., omitting workload details) [24, 45] or mod-"
ifying the query optimizer to return richer statistics [36]. Unfortu-
"nately, these approaches are not principled and thus do not neces-"
"sarily guarantee robustness. (In Section 6.4, we compare against"
commercial databases that use such heuristics.)
"To avoid these limitations, adaptive indexing schemes such as"
"Database Cracking [39, 43] take the other extreme by completely"
ignoring the past workload in deciding which indices to build; in-
"stead of an offline design, they incrementally create and refine in-"
"dices as queries arrive, on demand. However, even these techniques"
need to decide which subsets of columns to build an incremental in-
dex on.1 Instead of completely relying on past workloads or aban-
"doning the offline physical design, in this paper we present a princi-"
"pled framework for directly maximizing robustness, which enables"
users to decide on the extent to which they want to rely on past
"information, and the extent of uncertainty they want to be robust"
against. (We discuss the merits of previous work in Section 7.)
Our Approach — Recent breakthroughs in Operations Research
on robust optimization (RO) theory have created new hopes for
achieving robustness and optimality in a principled and tractable
"fashion [15, 16, 29, 75]. In this paper, we present the first attempt"
at applying RO theory to building a practical framework for solving
"one of the most fundamental problems in databases, namely find-"
"ing the best physical design. In particular, we study the effects of"
workload changes on query latency. Since OLTP workloads tend
"to be more predictable (e.g., transactions are often instances of a"
"few templates [57, 58]), we focus on OLAP workloads where ex-"
ploratory and ad-hoc queries are quite common. Developing this
robust framework is a departure from the traditional way of design-
ing and tuning databases: from today’s brittle designs to a princi-
pled world of robust designs that guarantee a predictable and con-
sistent performance.
"1Moreover, on-demand and continuous physical re-organizations"
"are not acceptable in many applications, which is why nearly all"
commercial databases still rely on their offline designers.
RO Theory — The field of RO has taken many strides over the
"past decade [15]. In particular, the seminal work of Bertsimas et al."
"[16] has been successfully applied to a number of drastically differ-"
"ent domains, from nano-photonic design of telescopes [16] to thin-"
film manufacturing [18] and system-on-chip architectures [60]. To
"the best of our knowledge, developing a principled framework for"
applying RO theory to physical design problems is the first appli-
"cation of these techniques in a database context, which involves a"
number of unique challenges not previously faced in any of these
other applications of RO theory (discussed in Section 4.2).
A common misconception about the RO framework is that it re-
"quires knowledge of the extent of uncertainty, e.g., in our case, an"
upper bound on how much the future workload will deviate from
"the past one.2 To the contrary, the power of the RO formulation"
is that it allows users to freely request any degree of robustness
"that they wish, say Γ, purely based on their own risk tolerance and"
"preferences [13, 17]. Regardless of whether the actual amount of"
"uncertainty exceeds or stays lower than Γ, the RO framework guar-"
"antees will remain valid; that is, the delivered design is promised to"
remain optimal as long as the uncertainty remains below the user-
"requested threshold Γ, and beyond that (i.e., if uncertainty exceeds"
"Γ) is in accordance to user’s accepted degree of risk [17]. In other"
"words, the beauty of RO theory is that it provides a framework"
for expressing and delivering reliability guarantees by decoupling
"them from the actual uncertainty in the environment (here, the fu-"
ture workload).
"Contributions — In this paper, we make these contributions:"
"• We formulate the problem of robust physical design using"
RO theory (Section 3).
"• We design a principled algorithm, called CliffGuard, by adapt-"
ing the state-of-the-art framework for solving non-convex
RO problems. CliffGuard’s design is generic and can poten-
tially work with any existing designers and databases without
modifying their internals (Section 4).
"• We implement and evaluate CliffGuard using two major com-"
mercial databases (HP Vertica and DBMS-X3) on two syn-
thetic workloads as well as a real workload of 430+K OLAP
queries issued by one of Vertica’s major customers over a
1-year period (Section 6).
"In summary, compared to Vertica’s state-of-the-art commercial de-"
"signer [47, 71], our robust designer reduces the average and max-"
imum latency of queries on average by 7× and 18× (and up to
"14× and 40×), respectively. Similarly, CliffGuard improves over"
DBMS-X by 3–5×. CliffGuard is currently available as an open-
"source, third-party tool [1]."
2. SYSTEM OVERVIEW
Physical Database Designs — A physical design in a database is
"a set of auxiliary structures, often built offline, which are used to"
speed up future queries as they arrive. The type of auxiliary struc-
tures used often depend on the specific database architecture. Most
2This misconception is caused by differing terminology used in
"other disciplines, such as mechanical engineering (ME) where “ro-"
bust optimization” refers to a different type of optimization which
requires some knowledge of the uncertainty of the physical envi-
ronment [32]. The Operations Research notion of RO used in this
paper is called reliability optimization in the ME literature [68].
"3DBMS-X is a major database system, which we cannot reveal due"
to the vendor’s restrictions on publishing performance results.
manner) and invoking its API again. CliffGuard repeats this pro-
"cess, until it is satisfied with the robustness of the design produced"
by the nominal designer. The final (robust) design is then sent back
"to the administrator, who may decide to deploy it in the DBMS."
3. PROBLEM FORMULATION
"In this section, we present a simple but powerful formulation of"
robustness in the context of physical database design. This formu-
lation will allow us to employ recently proposed ideas in the theory
of robust optimization (RO) and develop a principled and effective
"algorithm for finding robust database designs, which will be pre-"
"sented in Section 4. First, we define some notations."
"Notations — For a given database, the design space S is the set"
"of all possible structures of interest, such as indices on different"
"subsets of columns, materialized views, different samples of the"
"data, or a combination of these. For example, Vertica’s designer"
"[71] materializes a number of projections, each sorted differently:"
CREATE PROJECTION projection_name
"AS SELECT col1, col2, ..., colN"
FROM anchor_table
"ORDER BY col1’, col2’, ..., colK’;"
"Here, S is extremely large due to the exponential number of possi-"
"ble projections. Similarly, for decisions about building materialized"
"views or (secondary) indices, S will contain all such possible struc-"
tures. Existing database designers solve the following optimization
problem (or aim to4):
"nomD = D(W0, B) = ArgMin f(W0, D) (1)"
"D⊆S , price(D)≤B"
"where W0 is the target workload (e.g., the set of user queries), B"
"is a given budget (in terms of storage or maintenance overhead),"
D is a nominal designer that takes a workload and budget as input
"parameters, price(D) is the price of choosing D (e.g., the total"
"size of the projections in D), and f(W0, D) is our cost function"
"for executing workloadW0 using designD (e.g., f can be the query"
latency). We call such designs nominal as they are optimal for the
"nominal value of the given parameters (e.g., the target workload)."
"All existing designers [7, 31, 54, 71, 76] are nominal: they either"
"minimize the expression above directly, or follow other heuristics"
aimed at approximate minimization. Despite several heuristics to
"avoid over-fitting a given workload (e.g., omitting query details [24,"
"45]), nominal designers suffer from many shortcomings in practice;"
see Sections 1 and 6.4.
Robust Designs — This paper’s goal is finding designs that are
robust against worst-case scenarios that can arise from uncertain
situations. This concept of robustness can be illustrated using the
"toy example of Figure 2, which features a design space with only"
three possible designs and a toy workload that is represented by
a single real-value parameter μ. When our current estimate of μ
"is μ0, a nominal designer will pick design D1 since it minimizes"
the cost at μ0. But if we want a design that remains optimal even
"if our parameter changes by up to Γ, then a robust designer will"
"pick design D2 instead of D1, even though the latter has a lower"
cost at μ0. This is because the worst-case cost of D2 over the
"[μ0 − Γ, μ0 + Γ] is lower than that of D1; that is, D2 is robust"
"against uncertainty of up to Γ. Similarly, if we decide to guard"
"against a still greater degree of uncertainty, say for an estimation"
"error as high as Γ′ > Γ, a robust designer would this time pick"
"D3 instead of D2, as the former has a lower worst-case cost in"
"[μ − Γ′0 , μ0 + Γ′] than the other designs."
"4Existing designers often use heuristics or greedy strategies [55],"
which lead to approximations of the nominal optima.
Figure 1: The CliffGuard architecture.
databases use both materialized views and indices in their physical
designs. Materialized views are typically more common in analyt-
ical workloads. Approximate databases use small samples of the
data (rather than its entirety) to speed up query processing at the
"cost of accuracy [2, 3, 4, 23, 73, 74]. Physical designs in these"
"systems consist of different types of samples (e.g., stratified on dif-"
"ferent columns [5, 23]). Some modern columnar databases, such"
"as Vertica [71], build a number of column projections, each sorted"
"differently. Instead of traditional indices, Vertica chooses a projec-"
tion with the appropriate sort order (depending on the columns in
the query) in order to locate relevant tuples quickly. In all these
"examples, the space of these auxiliary structures is extremely large"
"if not infinite, e.g., there are O(2N · N !) possible projections or"
"indices for a table of N columns (i.e., different subsets and orders"
"of columns). Thus, the physical design problem is choosing a small"
"number of these structures using a fixed budget (in terms of time,"
"space, or maintenance overhead) such that the overall performance"
is optimized for a target workload.
Design Principles — A major goal in the design of our CliffGuard
algorithm is compatibility with almost any existing database in or-
"der to facilitate its adoption in the commercial world. Thus, we"
"have made two key decisions in our design. First, CliffGuard should"
operate alongside an existing (nominal) designer rather than replac-
"ing it. Despite their lack of robustness, existing designers are highly"
sophisticated tools hand-tuned over the years to find the best phys-
"ical designs efficiently, given their input parameters. Because of"
"this heavy investment, most vendors are reluctant to abandon these"
"tools completely. However, some vendors have expressed inter-"
est in CliffGuard as long as it can operate alongside their existing
"designer and improve its output. Second, CliffGuard is designed to"
"treat existing designers as a black-box (i.e., without modifying their"
internal implementations). This is to conform to the proprietary
nature of commercial designers and also to widen the applicabil-
ity of CliffGuard to different databases. By delegating the nominal
"designs to existing designers, CliffGuard remains a genetic frame-"
"work agnostic to the specific details of the design objects (e.g., they"
"can be materialized views, samples, indices, or projections)."
These design principles have already allowed us to evaluate Cliff-
Guard for two database products with drastically different design
"problems (i.e., Vertica and DBMS-X). Without requiring any changes"
"to their internal implementations, CliffGuard significantly improves"
on the sophisticated designers of these leading databases (see Sec-
"tion 6). Thus, we believe that CliffGuard can be easily used to"
speed up other database systems as well.
Architecture — Figure 1 depicts the high-level workflow of how
CliffGuard is to be used alongside a database system. The database
"administrator states her desired degree of robustness Γ to CliffGuard,"
which is located outside the DBMS. CliffGuard in turn invokes
the existing physical designer via its public API. After evaluating
"the output (nominal) design sent back from the existing designer,"
CliffGuard may decide to manipulate the existing designer’s output
by merely modifying some of its input parameters (in a principled
Algorithm 1: Generic robust optimization via gradient descent.
"Inputs: Γ: the radius of the uncertainty region,"
f(x): the cost of design x
"Output: x∗: a robust design, i.e.,"
x∗ = ArgMin Max f(x+ ∆x)
x ||∆x||2≤Γ
1 x1← pick an arbitrary vector // the initial decision
2 k ← 1 // k is the number of iterations so far
3 while true do
"// Neighborhood Exploration:"
5 U ← Find the set of worst-neighbors of xk within its
"Γ-neighborhood"
"// Robust Local Move:"
"∗7 ~d ← FindDescentDirection(xk, U )"
"// See Fig 3(a) for FindDescentDirection’s geometric"
intuition (formally defined in [59])
9 if there is no such direction ~d∗ pointing away from all u ∈ U
then
x∗11 ← xk // found a local robust solution
12 return x∗
else
14 tk ← choose an appropriate step size
15 x k+1 ← xk+tk·d ~ ∗// move along the descent direction
16 k ← k + 1 // go to next iteration
end
end
Finding the worst-neighbors — BNT relies on our ability to find
"the worst-neighborsU (Algorithm 1, Line 5) in each iteration, which"
equates to finding all global maxima of the following optimization
problem:
ArgMax f(xk + ∆x) (4)
"||∆x||2≤Γ"
"In other words, the worst-neighbors are defined as:"
"U = {xk + ∆x | f(xk + ∆x) = g(xk), ||∆x||2 ≤ Γ}"
"where g(x) represents our worst-case cost function, defined as:"
g(x) = Max f(x+ ∆x)
"||∆x||2≤Γ"
"When g(x) is differentiable, finding its global maxima is straight-"
"forward, as one can simply take its derivative and solve the follow-"
ing equation:
"′g (x) = 0 (5)"
All previous applications of the BNT framework have either in-
volved a closed form cost function f(x) with a differentiable worst-
"case cost function g(x), where the worst-neighbors can be found by"
"solving (5) (e.g., in industrial engineering [15] or chip design [60]),"
or a black-box cost function guaranteed to be continuously differ-
"entiable (e.g., in designing nano-photonic telescopes [16])."
"Challenge C2. Unfortunately, most cost functions of interest in"
"databases are not closed-form, differentiable, or even continuous."
"For instance, when f is the query latency, it does not have a closed-"
form; it is measured either via actual execution or by consulting the
"query optimizer’s cost estimates. Also, even a small modification"
"in the design or the query can cause a drastically different latency,"
"e.g., when a query references a column that is omitted from a ma-"
terialized view.
Finding a descent direction — BNT relies on our ability to effi-
ciently find the (steepest) descent direction via a second-order cone
program (SOCP) [16]. SOCPs require a continuous domain and
can be solved via interior point methods [19].
Challenge C3. We cannot use the same SOCP formulation be-
cause the space of physical designs is not continuous. A physical
"design, say a set of projections, can be easily encoded as a binary"
"vector. For instance, each projection can be represented as a vector"
"in {0, 1}m where the i’th coordinate represents the presence or ab-"
sence of the i’th column in the database. Different column-orders
and a set of such structures can also be encoded using more dimen-
"sions. However, this and other possible encodings of a database"
"design are inherently discrete. For instance, one cannot construct"
a conventional projection with only 0.3 of a column—a column is
either included in the projection or not.
Moving along a descent direction — BNT assumes that the deci-
"sion space (i.e., the domain of x) is continuous and hence, moving"
"along a descent direction is trivial (Algorithm 1, Line 15). In other"
"words, if xk is a valid decision, then xk + t k · d ~ ∗is also a valid"
decision for any given d∗ and tk > 0.
Challenge C4. Even when a descent direction is found in the
"database design space, moving along that direction does not have"
"any database equivalence. In other words, even when our vectors"
"x and ~d∗k correspond to legitimate physical designs, xk + tk ·"
"~d∗ may no longer be meaningful since it may not correspond to"
"any legitimate design, e.g., it may involve fractional coordinates for"
"some of the columns depending on the value of tk. Thus, we need"
to establish a different notion of moving along a descent direction
for database designs.
repeated from different starting points to find multiple local optima
and choose one that is more globally optimal.6)
4.2 Challenges of Applying BNT to Database
Problems
"As mentioned earlier, since BNT does not require a closed-form"
"cost function (or even convexity), it presents itself as the most ap-"
propriate technique in the RO literature for solving our physical de-
"sign problems, especially since we want to avoid modifying the in-"
"ternals of the existing designers (due to their proprietary nature, see"
"Section 2). However, BNT still hinges on certain key assumptions"
that prevent it from being directly applicable to our design prob-
"lem. Next, we discuss each of these requirements and the unique"
challenges that they pose in a database context.
Proper distance metric — BNT requires a proper distance metric
"over the decision space, i.e., one that is symmetric and satisfies the"
"triangle property. E.g., the L2-norm ||x||2 is a proper distance over"
"the m-dimensional Euclidean space, since ||x1 − x2||2 + ||x2 −"
"x3||2 ≥ ||x1 − x3||2 = ||x3 − x1||2 for any x1, x2, x ∈ Rm3 ."
Challenge C1. To define an analogous notion of uncertainty in a
"database context, we need to have a distance metric δ(W1,W2) for"
"any two sets of SQL queries, say W1 and W2, in order to express"
"the uncertainty set of an existing workloadW0 as {W | δ(W0,W ) ≤"
"Γ}. Note that δ must be symmetric, triangular, and also capable of"
capturing the user’s notion of a workload change. To the best of
"our knowledge, such a distance metric does not currently exist for"
database workloads.7
"6When f(x) is non-convex, the output of existing designers is also"
"a local optimum. Thus, even in this case, finding local robust op-"
tima is still preferable (to a local nominal optimum).
7While workload drift is well-observed in the database community
"[40, 64], quantifying it has received surprisingly little attention."
"In summary,",in,order,to,use BNT’s principled,"framework,",we
need to develop analogous techniques in our database context for,,,,,,
expressing distance and finding the worst-neighbors; we also need,,,,,,
to define equivalent notions for finding and moving along a descent,,,,,,
"direction. Next,",we explain how our CliffGuard algorithm over-,,,,,
comes challenges C1–C4 and uses BNT’s framework to find robust,,,,,,
physical database designs.,,,,,,
4.3 Our Algorithm: CliffGuard,,,,,,
"In this section, we propose our novel algorithm, called CliffGuard,",,,,,,
which builds upon BNT’s principled framework by tailoring it to,,,,,,
the problem of physical database design.,,,,,,
"Before presenting our algorithm,",,,,we need to clarify a few no-,,
"tional differences. Unlike BNT, where the cost function f(x) takes",,,,,,
"a single parameter x,",,the cost in CliffGuard is denoted as a two-,,,,
"parameter function f(W,D) where W",,,,is a given workload and D,,
"is a given physical design. In other words, each point x in our space",,,,,,
"is a pair of elements (W,D).",,,,"However, unlike BNT where vector",,
"x can be updated in its entirety, in CliffGuard (or any database de-",,,,,,
signer) we only update the design element D;,,,,this is because the,,
"database designer can propose a new physical design to the user,",,,,,,
but cannot impose a new workload on her as a means to improve,,,,,,
robustness.,,,,,,
Algorithm 2 presents the pseudocode for CliffGuard.,,,,,Like Al-,
"gorithm 1, Algorithm 2 iteratively explores a neighborhood to find",,,,,,
"the worst-neighbors,",,then,moves,farther away from,these neigh-,
bors in each iteration using an appropriate direction and step size.,,,,,,
"However, to apply these ideas in a database context (i.e., address-",,,,,,
"ing challenges C1–C4 from Section 4.2), Algorithm 2 differs from",,,,,,
Algorithm 1 in the following important ways.,,,,,,
"Initialization (Algorithm 2,",,,,Lines 1–2) — CliffGuard starts by,,
invoking the existing designer D,,,,to find a nominal design D,,for
the initial workload W0.,,,"(Later, D will be repeatedly replaced by",,,
designs that are more robust.) CliffGuard also creates a finite set of,,,,,,
"perturbed workloads P = {W1, · · · ,Wn} by sampling the work-",,,,,,
load space in the Γ-neighborhood of W0.,,,,"In other words,",given,
"a distance metric δ, we find n workloads W1, · · · ,Wn",,,,,such that,
"δ(Wi,W0) ≤ Γ for i = 1, 2, · · · , n.",,,,(Section 5 discusses how to,,
"define δ for database workloads, how to choose n, and how to sam-",,,,,,
ple the workload space efficiently.),,,,"Next, as in BNT, CliffGuard",,
starts an iterative search with a neighborhood exploration and a ro-,,,,,,
bust local move in each iteration.,,,,,,
"Neighborhood Exploration (Algorithm 2, Line 6) — To find the",,,,,,
"worst-neighbors, in CliffGuard we need to also take the current de-",,,,,,
"signD into account (i.e., the set of worst-case neighbors ofW0 will",,,,,,
depend on the physical design that we choose). Given that we can-,,,,,,
not rely on the differentiability (or even continuity) of our worst-,,,,,,
"case cost function (Challenge C2), we use the worst-case costs on",,,,,,
our sampled workloads P,,,a proxy; instead of solving,,,
"",,,Max,"f(W,D)",,(6)
"",,"δ(W,W0)≤Γ",,,,
we solve,,,,,,
"",,,"Max f(W,D)",,,(7)
"",,,W∈P,,,
Note that (7) cannot provide an unbiased approximation for (6) sim-,,,,,,
"ply because P is a finite sample, and finite samples lead to biased",,,,,,
estimates for extreme statistics such as min and max [70].,,,,,"Thus,",
we do not rely on the nominal value of (7) to evaluate the quality,,,,,,
of our design.,"Rather,",we use,,the solutions to (7) as a proxy,,to
guide our search in moving away from highly (though not neces-,,,,,,
sarily the most) expensive neighbors.,,,,"In our implementation, we",,
further mitigate this sampling bias by loosening our selection cri-
"terion to include all neighbors that have a high-enough cost (e.g.,"
top-K or top 20%) instead of only those that have the maximum
"cost. To implement this step, we simply enumerate each workload"
in P and measure its latency on the given design.
"Robust Local Move (Algorithm 2, Lines 8–15) — To find equiva-"
lent database notions for finding and moving along a descent direc-
"tion (C3 and C4), we use the following idea. The ultimate goal of"
finding and moving along a descent direction is to reduce the worst-
"case cost of the current design. In CliffGuard, we can achieve this"
goal directly by manipulating the existing designer by feeding it a
mixture of the existing workload and its worst-neighbors as a sin-
gle workload.8 The intuition is that since nominal designers (by
definition) produce designs that minimize the cost of their input
"workload, the cost of our previous worst-neighbors will no longer"
"be as high, which is equivalent to moving our design farther away"
from those worst-neighbors. The questions then are (i) how do we
"mix these workloads, and (ii) what if the designer’s output leads to"
a higher worst-case cost?
"The answer to question (i) is a weighted union, where we take"
the union of all the queries in the original workload as well as those
"in the worst-neighbors, after weighting the latter queries according"
"to a scaling factor α, their individual frequencies of occurrence in"
"their workload, and their latencies against the current design. Tak-"
ing latencies and frequencies into account encourages the nominal
designer to seek designs that reduce the cost of more expensive
"and/or popular queries. Scaling factor α, which serves the same"
"purpose as step-size in BNT, allows CliffGuard to control the dis-"
tance of movement away from the worst-neighbors.
"We also need to address question (ii) because unlike BNT, where"
the step size tk could be computed to ensure a reduction in the
"worst cost, here our α factor may in fact lead to a worse design"
"(e.g., by moving too far from the original workload). To solve"
"this problem, CliffGuard dynamically adjusts the step-size using"
"a common technique called backtracking line search [19], similar"
to a binary-search. Each time the algorithm succeeds in moving
"away from the worst-neighbors, we consider a larger step size (by"
a factor λsuccess >1) to speed up the search towards the robust so-
"lution, and each time we fail, we reduce the step size (by a factor"
0< λfailure <1) as we may have moved past the robust solution
"(hence observing a higher worst-case cost)."
"Termination (Algorithm 2, Lines 17–20) — We repeat this pro-"
cess until we find a local robust optimum (or reach the maximum
"number of steps, when under a time constraint)."
5. EXPRESSING ROBUSTNESS GUARAN-
TEES
"In this section, we define a database-specific distance metric δ"
so that users can express their robustness requirements by specify-
"ing a Γ-neighborhood (as an uncertainty set, described in Section 3)"
"around a given workloadW0, and demanding that their design must"
"be robust for any future workload W as long as δ(W0,W ) ≤ Γ."
"Thus, users can demand arbitrary degrees of robustness according"
to their performance requirements. For mission-critical applica-
"tions more sensitive to sudden performance drops, users can be"
"more conservative (specifying a larger Γ). At the other extreme,"
users expecting no change (or less sensitive to it) can fall back to
the nominal case (Γ = 0).
8Remember that existing designers only take a single workload as
their input parameter.
Algorithm 2: The CliffGuard algorithm.
"Inputs: Γ: the desired degree of robustness,"
"δ: a distance metric defined over pairs of workloads,"
"W0: initial workload,"
"D: an existing (nominal) designer,"
"f : the cost function (or its estimate),"
"Output: D∗: a robust design, i.e., D∗ = ArgMin Max f(W,D)"
D δ(W−W0)≤Γ
1 D ← D(W0) // Invoke the existing designer to find a nominal design for W0
"2 P ← {Wi | 1 ≤ i ≤ n, δ(Wi,W ) ≤ Γ} // Sample some perturbed workloads in the Γ-neighbor of W0"
3 Pick some α > 0 // some initial size for the descending steps
4 while true do
"// Neighborhood Exploration:"
"6 U ← {W̃1, · · · , W̃m} where W̃i∈P and f (W̃i, D)= Max f (W ,D) // Pick perturbed workloads with the worst performance onD"
W∈P
"// Robust Local Move:"
"8 Wmoved ←MoveWorkload(W0, {W̃1, · · · , W̃m}, f,D, α) //Build a new workload by moving closer to W0’s worst-neighbors"
"(see Alg. 3)"
9 D′ ← D(Wmoved) // consider the nominal design for Wmoved as an alternative design
"10 if Max f(W,D′) < Max f(W,D) // Does D′ improve on the existing design in terms of the worst-case performance?"
W∈P W∈P
then
"′12 D ← D // Take D′ as your new design"
13 α← α ∗ λsuccess (for some λsuccess > 1) // increase the step size for the next move along the descent direction
else
15 α← α ∗ λfailure (for some λfailure < 1) // consider a smaller step next time
end
17 if your time budget is exhausted or many iterations have gone with no improvements
then
D∗ ← D // the current design is robust
"∗20 return D"
end
end
"SQL expressions, query plans, or latencies are substantially differ-"
"ent. Using this representation, there will be only 2n − 1 possible"
queries where n is the total number of columns in the database
"(including all the tables). (Here, we ignore queries that do not ref-"
"erence any columns.) Thus, we can represent a workload W with"
"a (2n − 1)-dimensional vector VW = 〈r1, · · · , r2n−1〉 where ri"
represents the normalized frequency of queries that are represented
"by the i’th subset of the columns for i = 1, · · · , 2n − 1. With this"
"notation, we can now introduce our Euclidean distance for database"
workloads as:
"Tδeuclidean(W1,W2) = |VW − VW | × S × |VW − VW | (9)1 2 1 2"
"Here, S is a (2n − 1) × (2n − 1) similarity matrix, and thus"
"δeuclidean is always a real-valued number (i.e., 1×1 matrix). Each"
"Si,j entry is defined as the total number of columns that are present"
"only in qi or qj (but not in both), divided by 2 · n. In other words,"
"Si,j is the Hamming distance between the binary representations of"
"i and j, divided by 2 ·n. Hamming distances are divided by 2 ·n to"
"ensure a normalized distance, i.e., 0 ≤ δeuclidean(W1,W2) ≤ 1."
"One can easily verify that δeuclidean satisfies criteria (b), (c), and"
"(d). In Section 6.3, we empirically show that this distance metric"
"also satisfies criterion (a) quite well. Finally, even though VW is"
"exponential in the number of columns n, it is merely a conceptual"
"model; since VW is an extremely sparse matrix, most of the com-"
"putation in (9) can be avoided. In fact, δeuclidean can be computed"
"in O(T 2 · n) time and memory complexity, where T is the number"
"of input queries (e.g., in a given query log)."
A distance metric δ must satisfy the following criteria to be ef-
fectively used in our BNT-based framework (the intuition behind
these requirements can be found in our technical report [59]):
"R1. Soundness, which requires that the smaller the distance δ(W1,"
the better the performance of W2 on W1’s nominally optimal
"design. Formally, we call a distance metric sound if it satisfies:"
"δ(W1,W2)≤δ(W1,W3)⇒f(W2,D(W1)) ≤ f(W3,D(W1))"
"(8)"
"R2. δ should account for intra-query similarities; that is, if r1 > r2i i"
"and r1 < r2j j , the distance δ(W1,W2) should become smaller"
"based on the similarity of the queries qi and qj , assuming the"
same frequencies for the other queries.
"R3. δ should be symmetric; that is, δ(W1,W2) = δ(W2,W1) for"
anyW1 andW2. (This is needed for the theoretical guarantees
of the BNT framework.)
"R4. δ must satisfy the triangular property; that is, δ(W1,W2) ≤"
"δ(W1,W3) + δ(W3,W2) for any W1,W2,W3. (This is an"
implicit assumption in almost all gradient-based optimization
"techniques, including BNT.)"
"Before introducing a distance metric fulfilling these criteria, we"
need to introduce some notations. Let us represent each query as
"the union of all the columns that appear in it (e.g., unioning all the"
"columns in the select, where, group by, and order by clauses)."
"With this over-simplification, two queries will be considered iden-"
"tical as long as they reference the same set of columns, even if their"
Algorithm 3: The subroutine for moving a workload.
"Inputs: W0: an initial workload,"
"{W̃1, · · · , W̃m}: workloads to merge with W0,"
"f : the cost function (or its estimate),"
"D: a given design,"
"α: a scaling factor for the weight (α > 0)"
Output: Wmoved: a new (merged) workload which is closer to
"{W̃1, · · · , W̃N} than W0, i.e.,"
"Σiδ(W̃i,Wmoved) < Σiδ(W̃i,W0)"
"Subroutine MoveWorkload (W0, {W̃1, · · · , W̃m}, f,D, α)"
2 Wmoved ← {}
"3 Q← the set of all queries in W0 and W̃1, · · · , W̃m"
workloads
4 foreach query q ∈ Q do
"5 α6 fq ← f({q}∑, D) D ωq ← (fq · m + weight(q,W0)i=1weight(q, W̃i)) // the cost of query q using design"
"7 Wmoved ←Wmoved ∪ {(q, ωq)}"
end
9 return Wmoved
end
"Limitations — δeuclidean has a few limitations. First, it does not"
"factor in the clause in which a column appears. For instance, for"
"fast filtering, it is more important for a materialized view to cover"
a column appearing in the where clause than one appearing only in
"the select clause. This limitation, however, can be easily resolved"
"by representing each query as a 4-tuple 〈v1, v2, v3, v4〉 where v1 is"
the set of columns in the select clause and so on. We refer to this
"distance as δseparate, as we keep columns appearing in different"
clauses separate. δseparate differs from δeuclidean only in that it
"creates 4-tuple vectors, but it is still computed using Equation (9)."
The second (and more important) limitation is that δeuclidean
may ignore important aspects of the SQL expression if they do not
"change the column sets. For example, presence of a join operator"
or using a different query plan can heavily impact the execution
"time, but are not captured by δeuclidean. In fact, as a stricter version"
"of requirement (8), a better distance metric will be one that for all"
"workloads W1,W2,W3 and arbitrary design D satisfies:"
"δ(W1,W2) ≤ δ(W1,W3) ⇒ (10)"
"|f(W2, D)− f(W1, D)| ≤ |f(W3, D)− f(W1, D)|"
"In other words, the distance functions should directly match the"
performance characteristics of the workloads (the lower their dis-
"tance, the more similar their performance). In Appendix C, we in-"
"troduce a more advanced metric that aims to satisfy (11). However,"
"in our experiments, we still use δeuclidean for three reasons."
"First, requirement (11) is unnecessary for our purposes. CliffGuard"
only relies on this distance metric during the neighborhood explo-
ration and feeds actual SQL queries (and not just their column
"sets) into the existing designer. Internally, the existing designer"
"compares the actual latency of different SQL queries, accounting"
"for their different plans, joins, and all other details of every input"
"query. For example, the designer ignores the less expensive queries"
to spend its budget on the more expensive ones.
"Second, we must be able to efficiently sample the Γ-neighborhood"
"of a given workload (see Algorithm 2, Line 2), which we can do"
when our cost function is δeuclidean. The sampling algorithm (which
can be found in Appendix B) becomes computationally prohibitive
when our distance metric involves computing the latency of differ-
"ent queries. In Section 6, we thoroughly evaluate our CliffGuard"
"algorithm overall, and our distance function in particular."
"The third, and final, reason is that the sole goal of our distance"
metric is to provide users a means to express and receive their de-
sired degree of robustness. We show that despite its simplistic na-
"ture, δeuclidean is still quite effective in satisfying (8) (see Section"
"6.3), and most importantly in enabling CliffGuard to achieve deci-"
sive superiority over existing designers (see Section 6.4).
"In the end, we note that quantifying the amount of change in"
SQL workloads is a research direction that will likely find many
"other applications beyond robust physical designs, e.g., in workload"
"monitoring [40, 64], auto-tuning [31], or simply studying database"
usage patterns. We believe that δeuclidean is merely a starting point
in the development of more advanced and application-specific dis-
tance metrics for database workloads.
6. EXPERIMENTAL RESULTS
The purpose of our experiments in this section is to demonstrate
that (i) real world workloads can vary over time and be subject to
"a great deal of uncertainty (Section 6.2), (ii) despite its simplic-"
"ity, our distance metric δeuclidean can reasonably capture the per-"
"formance implications of a changing workload (Section 6.3), and"
most importantly (iii) our robust design formulation and algorithm
improve the performance of the state-of-the-art industrial design-
"ers by up to an order of magnitude, without having to modify the"
internal implementations of these commercial tools (Section 6.4).
We also study different degrees of robustness (Section 6.5). Addi-
"tional experiments are deferred to Appendix A, where we evaluate"
the effects of different distance functions and other parameters on
"CliffGuard’s performance, and show that CliffGuard’s processing"
overhead is negligible compared to that of the deployment phase.
6.1 Experimental Setup
We have implemented CliffGuard in Java. We tested our algo-
rithm against Vertica’s database designer (called DBD [71]) and
DBMS-X’s designer as two of the most heavily-used state-of-the-
"art commercial designers, as well as two other baseline algorithms"
"(introduced later in this section). For Vertica experiments, we used"
its community edition and invoked its DBD and query optimizer
"via a JDBC driver. Similarly, we used DBMS-X’s latest API. We"
ran each experiment on two machines: a server and a client. The
server ran a copy of the database and was used for testing different
designs. The client was used for invoking the designer and sending
queries to the server. We ran the Vertica experiments on two Dell
"machines running Red Hat Enterprise Linux 6.5, each with two"
quad-core Intel Xeon 2.10GHz processors. One of the machines
had 128GB memory and 8×4TB 7.2K RPM disks (used as server)
and the other had 64GB memory and 4 × 4TB 7.2K RPM disks.
"For DBMS-X experiments, we used two Azure Standard Tier A3"
"instances, each with a quad-core AMD Opteron 4171 HE 2.10GHz"
"processor, 7GB memory, and 126GB virtual disks. In this section,"
"when not specified, we refer to our Vertica experiments."
Workloads9 — We conducted our experiments on a real-world
"(R1) workload and two synthetic ones (S1 and S2). R1 belongs"
"to one of the largest customers of the Vertica database, composed"
of 310 tables and 430+K time-stamped queries issued between
March 2011 and April 2012 out of which 15.5K queries conform
"to their latest schema (i.e., can be parsed). We did not have access"
to their original dataset but we did have access to their data distri-
"bution, which we used to generate a 151GB dataset for our Vertica"
"9Common benchmarks (e.g., TPC-H) are not applicable here as"
"they only contain a few queries, and do not change over time."
Table 1: Summary of our real-world and synthetic workloads.
experiments. Since we did not have access to any real workloads
"from DBMS-X’s customers, we used the same query log but on a"
smaller dataset (20GB) given the smaller memory capacity of our
Azure instances (compared to our Dell servers). We also created
"two synthetic workloads, called S1 and S2, as follows. We used"
"the same schema and dataset as R1, but chose different subsets"
and relative ordering of R1 queries to artificially cause different
degrees of workload change. Table 1 reports basic statistics on the
amount workload changes (in terms of δeuclidean) between consec-
utive windows of queries where each window was 28 days (differ-
ent window sizes are studied in Section 6.2). S1 queries were cho-
sen to mimic a workload with minimal change over time (between
"0.1m and m, where m is the minimum change observed in R1)."
S2 queries were chosen to exhibit the same range of δeuclidean as
R1 but more uniformly. More detailed analysis of these workloads
will be presented in the subsequent sections.
Algorithms Compared — We divided the queries according to
"their timestamps into 4-week windows,W0,W1, · · · . We re-designe"
the database at the end of each month to simulate a tuning fre-
"quency of a month (a common practice, based on our oral con-"
"versations). In other words, we fed Wi queries into each of the"
following designers and used the produced design to processWi+1
"(except for FutureKnowingDesigner; see below)."
"1. NoDesign: A dummy designer that returns an empty design (i.e.,"
no projections). Using NoDesign all queries simply scan the default
"super-projections (which contain all the columns), providing an up-"
per limit on each query’s latency.
2. ExistingDesigner: The nominal designer shipped with com-
"mercial databases. For instance, Vertica’s DBD [71] recommends"
a set of projections while DBMS-X’s designer finds various types
of indices and materialized views. We used these state-of-the-art
designers as our main baselines.
"3. FutureKnowingDesigner: The same designer as ExistingDesigner,"
"except that instead of feeding queries fromWi and testing onWi+1,"
we both feed and test it on Wi+1. This designer signifies the best
performance achievable where the designer knows exactly which
queries to expect in the future and optimize for.
4. MajorityVoteDesigner: A designer that uses sensitivity analysis
to identify elements of the nominal design that are brittle against
changes of workload. This designer uses the same technique as
"CliffGuard to explore the local neighborhood of the current Wi,"
"and generate a set of perturbed workloads W 1i , · · · ,Wni . Then, it"
invokes the ExistingDesigner to suggest an optimal design for each
"jWi . Finally, for each structure (e.g., index, materialized view, pro-"
"jection) s, MajorityVoteDesigner counts the number of times that"
"s has appeared in the nominal design of the neighbors, and selects"
those structures that have appeared in different designs most fre-
quently. The idea behind this heuristic is that structures that appear
in the optimal design of fewer neighbors (have fewer votes) are less
likely to remain beneficial when the future workload changes.
"5. OptimalLocalSearchDesigner: Similar to MajorityVoteDesigner,"
this designer starts by searching the local neighborhood of the given
"workload and generating perturbed workloads. However, instead"
"Figure 5: Many workloads drift over time (15.5K queries, 6 months)."
of selecting structures that have been voted for by the most number
"of neighbors, this designer takes the union of the queries in the"
"neighboring workloads as the expectation (i.e., representative) of"
"the future workload, say W̄ . This algorithm then solves an Integer"
Linear Program to find an optimal set of structures that fit in the
budget and minimize the cost of W̄ .10
7. CliffGuard: Our robust database designer from Section 4.
Note that DBD and DBMS-X’s designer (ExistingDesigner) are
our goal standards as the state-of-the-art designers currently used in
"the industry. However, we also aim to answer the following ques-"
tion. How much of CliffGuard’s overall improvement over nomi-
nal designers is due to its exploration of the initial workload’s local
"neighborhood, and how much is due to its carefully selected de-"
scent direction and step sizes in moving away from the worst neigh-
bors? Since MajorityVoteDesigner and OptimalLocalSearchDesigner
use the same neighborhood sampling strategy as CliffGuard but
"employ greedy and local search heuristics, we will be able to break"
down the contribution of CliffGuard’s various components to its
overall performance.
Since Vertica automatically decides on the storage budget (50GB
"in our case), we used the same budget for the other algorithms too."
"For DBMS-X experiments, we used a maximum budget of 10GB"
"(since the dataset was smaller). Also, unless otherwise specified,"
"we used n=20 samples in all algorithms involving sampling, and 5"
"iterations, λsuccess = 5, and λsuccess = 0.5 in CliffGuard."
6.2 Workloads Change Over Time
"First, we studied if and how much our real workload has changed"
over time. While OLTP and reporting queries tend to be more
repetitive (often instantiated from a few templates with different
"parameters), analytical and exploratory workloads tend to be less"
"predictable (e.g., Hive queries at Facebook are reported to access"
"over 200–450 different subsets of columns [5]). Likewise, in our"
"analytical workload R1, we observed that queries issued by users"
"have constantly drifted over time, perhaps due to the changing na-"
ture of their company’s business needs.
Figure 6.1 shows the percentage of queries that belonged to tem-
plates that were shared among each pair of windows as the time lag
"grew between the two windows. Here, we have defined templates"
by stripping away the query details except for the sets of columns
"used in the select, where, group by, and order by clauses. This"
is an overly optimistic analysis assuming that queries with the same
column sets in their respective clauses will exhibit a similar perfor-
"mance. However, even with this optimistic assumption, we ob-"
"served that for a window size of one week, on average only 51% of"
the queries had a similar counterpart between consecutive weeks.
This percentage was only 35% when our window was 4 weeks.
"Regardless of the window size, this commonality drops quickly as"
"the time lag increases, e.g., after 2.5 months less than 10% of the"
10A greedy version of this algorithm and a detailed description of
the other baselines can be found in our technical report [59].
non-convex — and as such can easily delude a designer into a lo-
"cal optimum. Thus, by avoiding the proximity of bad neighbors,"
CliffGuard seems to find designs that are also more globally opti-
"mal. In fact, for S2, Figure 7(c) shows that CliffGuard is only 30%"
"worse than a hypothetical, ideal world where future queries are"
"precisely known in advance (i.e., the FutureKnowingDesigner). For"
"S1, however, CliffGuard’s improvement over ExistingDesigner is"
more modest: 1.5× improvement for worst-case latency and 1.2×
for average latency. This is completely expected since S1 is de-
signed to exhibit no or little change between different windows
"(refer to Table 1). This is the ideal case for a nominal designer"
since the amount of uncertainty across workloads is so negligible
that even our hypothetical FutureKnowingDesigner cannot improve
"much on the nominal designer. Thus, averaging over all three work-"
"loads, compared to ExistingDesigner, CliffGuard improves the av-"
"erage and worst-case latencies by 6.9× and 18.3×, respectively."
Figure 10 reports a similar experiment for workload R1 but for
DBMS-X. (DBMS-X experiments on workloads S1 and S2 can be
found in Appendix A.3.) Even though DBMS-X’s designer has
"been fine-tuned and optimized over the years, CliffGuard still im-"
proves its worst-case and average-case performances by 2.5–5.2×
"and 2–3.2×, respectively. This is quite encouraging given that"
CliffGuard is still in its infancy stage of development and treats the
"database as a black-box. While still significant, the improvements"
here are smaller than those observed with Vertica. This is due to
several heuristics used in DBMS-X’s designer (such as omitting
workload details) that prevent it from overfitting its input work-
"load. However, this also shows that dealing with such uncertainties"
in a principled framework can be much more effective.
These experiments confirm our hypothesis that failing to account
for workload uncertainty can have significant consequences. For
"example, for R1 on Vertica, ExistingDesigner is on average only"
25% better than NoDesign (with no advantage for the worst-case).
"Note that here the database was re-designed every month, which"
means even this slight advantage of ExistingDesigner over NoDesign
would quickly fade away if the database were to be re-designed
less frequently (as the distance between windows often increases
with time; see Figure 6.1). These experiments show the ample im-
portance of re-thinking and re-architecting the existing designers
currently shipped and used in our database systems.
6.5 Effect of Robustness Knob on Performance
"To study the effect of different levels of robustness, we varied"
the Γ parameter in our algorithm and measured the average and
worst-case performances in each case. The results of this experi-
"ment for workloads R1 and S2 are shown in Figures 8 and 9, respec-"
"tively. (As reported in Section 6.4, workload S1 contains minimal"
"uncertainty and thus is ruled out from this experiment, i.e., the per-"
formance difference between ExistingDesigner and CliffGuard re-
"mains small for S1). Here, experiments on both workloads confirm"
that requesting a large level of robustness will force CliffGuard
"to be overly conservative, eliminating its margin of improvement"
over ExistingDesigner. Note that in either case CliffGuard still
"performs no worse than ExistingDesigner, which is due to two"
"reasons. First, ExistingDesigner is only marginally better than"
"NoDesign (refer to Section 6.4) and as Γ increases, its relevance"
for the actual workload (which has a much lower δeuclidean) de-
"grades. As a result, both designers approach NoDesign’s perfor-"
"mance, which serves an upper bound on latency (i.e., unlike theory,"
"latencies are always bounded in practice, due to the finite cost of the"
"worst query plan). The second reason is that, during each iteration"
"of CliffGuard (unlike BNT), our new workload always contains"
"the original workload which ensures that even when Γ is large, the"
"",Distance$between$W$and$W0
Figure 6:,Performance decay of a window W on a design made for
another window W0 is highly correlated with their distance.,
queries had,similar templates appearing in the past. The unpre-
dictability of analytical workloads underlines the important role of,
a robust designer.,We show in Section 6.4 that failing to take into
"account this potential change (i.e., uncertainty) in our target work-",
load has a severe impact on the performance of existing physical,
designers — one that we aim to overcome via our robust designs.,
6.3 Our Distance Metric Is Sound,
In Section,"5, we introduced our distance metric δeuclidean to"
concisely quantify the dissimilarity of two SQL workloads. While,
"we do not claim that δeuclidean is an ideal one (see Section 5), here",
"we show that it is sound. That is, in general:",
"′δ(W0,W )","≤ δ(W0,W ′ )⇒ f(W,D(W0)) ≤ f(W ,D(W0))"
which means that a design made for W0,is more suitable for W
"than it is for W ′, i.e., W will experience a lower latency than W ′.",
Figure 6 reports an experiment where we chose 10 different start-,
ing windows,as our W0 and created a number of windows with
different distances from W0.,The curve (error bar) shows the av-
erage (range) of the latencies of these different windows for each,
distance.,This plot indicates a strong correlation and monotonic
relationship between performance decay and δeuclidean.,"Later, in"
"Section 6.4, we show that even with this simplistic distance metric,",
our CliffGuard algorithm can consistently improve on Vertica’s lat-,
est designer by severalfold.,
6.4 Quality of Robust vs. Nominal Designs,
"In this section, we turn to the most important questions of this",
paper: is our robust designer superior to state-of-the-art designers?,
"And, if so,",by what measure? We compared these designers us-
ing all 3 workloads.,"In R1, out of the 15.5K queries, only 515"
could benefit,"from a physical design, i.e., the remaining queries"
"were either trivial (e.g., select version()) or returned an entire",
"table (e.g.,",‘select * from T’ queries with no filtering used for
backup purposes) in which case they always took the same time as,
they only used the super-projections in Vertica and table-scans in,
"DBMS-X. Thus,",we only considered queries for which there ex-
isted an ideal design (no matter how expensive) that could improve,
on their bare table-scan latency by at least a factor of 3×.,
Figure 7,summarizes the results of our performance compari-
"son on Vertica, showing the average and maximum latencies (both",
averaged over all windows) for all,"three workloads. On average,"
"MajorityVoteDesigner improved on the existing designer by 13%,",
while OptimalLocalSearchDesigner’s performance was slightly worse,
than Vertica’s DBD.,"However, CliffGuard was superior to the ex-"
isting designer by an astonishing margin:,"on average, it cut down"
the maximum latency of each window by 39.7× and 13.7× for R1,
"and S2, respectively. Interestingly, for these workloads, even Cliff-",
Guard’s average-case performance was 14.3× and 5.3× faster than,
ExistingDesigner. The last result is surprising because our CliffGuard,
is designed,to protect against worst-case scenarios and ensure a
predictable performance.,"However, improvement even on the av-"
erage case indicates that the design space of a database is highly,
"Figure 7: Average and worst-case performances of designers for Vertica, averaged over all windows, for workloads R1, S1, and S2."
designer will not completely ignore the original workload (see Al-
"gorithm 3). Also, as expected, as Γ approaches zero, CliffGuard’s"
performance again approaches that of a nominal designer.
7. RELATED WORK
There has been much research on physical database design prob-
"lems, such as the automatic selection of materialized views [42, 51,"
"65, 72], indices [27, 54, 61, 69], or both [7, 31, 46, 76]. Also, most"
"modern databases come with designer tools, e.g., Tuning Wizard"
"in Microsoft SQL Server [7], IBM DB2’s Design Advisor [76],"
"Teradata’s Index Wizard [20], and Oracle’s SQL Tuning Adviser"
"[31]. Other types of design problems include project selection in"
"columnar databases [33, 47, 71], stratified sample selection in ap-"
"proximate databases [3, 5, 12, 23], and optimizing different repli-"
cas for different workloads in a replicated databases [67]. All these
designers are nominal and assume that their target workload is pre-
"cisely known. Since future queries are often not known in advance,"
these tools optimize for past queries as approximations of future
ones. By failing to take into account the fact that a portion of those
"queries will be different in the future, they produce designs that are"
sub-optimal and brittle in practice. To mitigate some of these prob-
"lems, a few heuristics [28] have been proposed to compress and"
"summarize the workload [24, 45] or modify the query optimizer to"
"produce richer statistics [36]. However, these approaches are not"
"principled and thus, do not necessarily guarantee robustness. In"
"contract, CliffGuard takes the possible changes of workload into"
"account in a principled manner, and directly maximizes the robust-"
ness of the physical design.
"To avoid these limitations, adaptive indexing schemes [37, 38,"
"39, 44, 64]) take the other extreme by avoiding the offline phys-"
"ical design, and instead, creating and adjusting indices incremen-"
"tally, on demand. Despite their many merits, these schemes do not"
have a mechanism to incorporate prior knowledge under a bounded
"amount of uncertainty. Also, one still needs to decide which sub-"
"sets of columns to build an adaptive index on. For these reasons,"
most commercial databases still rely on their offline designers. In
"contrast, CliffGuard uses RO theory to directly minimize the effect"
"of uncertainty on optimality, and guarantee robustness."
The effect,of,uncertainty,(caused,by,cost,and,cardinality,esti-
mates) has also been studied in the context of query optimization,,,,,,,,
"[11, 25, 30, 35, 56, 63] and choosing query plans with a bounded",,,,,,,,
worst-case [10].,,None of these studies have addressed uncertain-,,,,,,
"ties caused by workload changes, or their impact on physical de-",,,,,,,,
"signs. Also, while these approaches produce plans that are more",,,,,,,,
"predictable, they are not principled in that they do not directly max-",,,,,,,,
"imize robustness, i.e., they do not guarantee robustness even in the",,,,,,,,
"context of query optimization. Finally, most of these heuristics are",,,,,,,,
specific to a particular problem and do not generalize to othersD. BMSMX',,,,,,,,
Theory of robust optimization has taken many strides in recent,,,,,,,,
"years [15, 16, 17, 29, 34, 48, 75] and has been applied to many other",,,,,,,,
"disciplines, e.g.,",,"supply chain management [15],",,,,,circuit [62] and,
"antenna [53] design, power control [41], control theory [14], thin-",,,,,,,,
film manufacturing,,"[18], and microchip architecture",,,,,[60]. How-,
"ever, to the best of our knowledge, this paper is the first application",,,,,,,,
of RO theory in a database context (see Section 4.2).,,,,,,,,
8. CONCLUSION AND FUTURE WORK,,,,,,,,
The state-of-the-art database designers rely on heuristics that do,,,,,,,,
not account for,,"uncertainty,",and hence,,produce,,sub-optimal,and
"brittle designs. On the other hand, the principled framework of ro-",,,,,,,,
"bust optimization theory, which has witnessed remarkable advances",,,,,,,,
over the past,"few years,",has,largely dealt with problems that are,,,,,
"quite different in nature than those faced in databases. In this paper,",,,,,,,,
we presented CliffGuard to exploit these techniques in the context,,,,,,,,
of physical design problems in a columnar database. We compared,,,,,,,,
our algorithm to a state-of-the-art commercial designer using sev-,,,,,,,,
"eral real-world and synthetic workloads. In summary, compared to",,,,,,,,
"Vertica’s state-of-the-art designer, our robust designer reduces the",,,,,,,,
"average and maximum latency of queries by up to 5× and 11×,",,,,,,,,
respectively.,"Similarly, CliffGuard improves upon DBMS-X’s de-",,,,,,,
signer by 3–5×.,,Since CliffGuard treats the existing designer as,,,,,,
"a block-box,",with,no modifications,,to the,database,,"internals,",an
interesting future direction is to extend CliffGuard to other major,,,,,,,,
DBMSs with other types of design problems.,,,,,,,,
Acknowledgements,,,,
This work is in part supported by,Amazon AWS,and,Microsoft,
Azure. The authors are grateful to,the anonymous reviewers,,,for
"their insightful feedback, Michael Stonebraker and Samuel Mad-",,,,
"den for their early contributions, Stephen Tu for his SQL parser,",,,,
Shiyong Hu and Ruizhi Deng for implementing our distance met-,,,,
"rics, Yingying Zhu for plotting Figure 4, Andrew Lamb and Vivek",,,,
Bharathan for helping with Vertica,"experiments, and",Alice,Tsay,
and Suchee Shah for their comments on this manuscript.,,,,
