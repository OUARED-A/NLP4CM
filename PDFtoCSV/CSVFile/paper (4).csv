Cost-Model Oblivious Database Tuning
with Reinforcement Learning
"Abstract. In this paper, we propose a learning approach to adaptive"
performance tuning of database applications. The objective is to validate
the opportunity to devise a tuning strategy that does not need prior
"knowledge of a cost model. Instead, the cost model is learned through"
reinforcement learning. We instantiate our approach to the use case of
index tuning. We model the execution of queries and updates as a Markov
"decision process whose states are database configurations, actions are"
"configuration changes, and rewards are functions of the cost of config-"
uration change and query and update evaluation. During the reinforce-
"ment learning process, we face two important challenges: not only the"
"unavailability of a cost model, but also the size of the state space. To"
"address the latter, we devise strategies to prune the state space, both"
in the general case and for the use case of index tuning. We empirically
and comparatively evaluate our approach on a standard OLTP dataset.
We show that our approach is competitive with state-of-the-art adaptive
"index tuning, which is dependent on a cost model."
"In a recent SIGMOD blog entry [10], Guy Lohman asked “Is query optimization"
a ‘solved’ problem?”. He argued that current query optimizers and their cost
"models can be critically wrong. Instead of relying on wrong cost models, the"
author and his colleagues have proposed in [19] a learning optimizer.
"In this paper, we propose a learning approach to performance tuning of data-"
"base applications. By performance tuning, we mean selection of an optimal phys-"
"ical database configuration in view of the workload. In general, configurations"
"differ in the indexes, materialized views, partitions, replicas, and other para-"
"meters. While most existing tuning systems and literature [6,17,18] rely on a"
"predefined cost model, the objective of this work is to validate the opportunity"
for a tuning strategy to do without.
"To achieve this, we propose a formulation of database tuning as a reinforce-"
ment learning problem (see Sect. 3). The execution of queries and updates is
"modeled as a Markov decision process whose states are database configurations,"
actions are configuration changes and rewards are functions of the cost of con-
figuration change and query/update evaluation. This formulation does not rely
"on a pre-existing cost model, rather it learns it."
We present a solution to the reinforcement learning formulation that tackles
"the curse of dimensionality (Sect. 4). To do this, we reduce the search space by"
"exploiting the quasi-metric properties of the configuration change cost, and we"
approximate the cumulative cost with a linear model.
We instantiate our approach to the use case of index tuning (Sect. 5). We use
this case to demonstrate the validity of a cost-model oblivious database tun-
"ing with reinforcement learning, through experimental evaluation on a TPC-C"
workload [14] (see Sect. 6). We compare the performance with the Work Func-
tion Index Tuning (WFIT) Algorithm [18]. Results show that our approach is
competitive yet does not need to know a cost model.
Related work is discussed in Sect. 2.
Our work is intertwined with mainly two lines of research. Our methodology is
designed to deal with the problem of automated database configuration. Using
"our approach described in Sect. 4, we have proposed COREIL, an algorithm to"
"solve the problem of index tuning. Traditionally, most of the works proposed"
in the field of automated database configuration are conducted in an offline
"manner. In offline methodologies, database administrators identify and update"
representative workloads from the database queries based on these representative
"workloads, new database configurations are realized to create new beneficial"
"indexes [1], smart vertical partition for reducing I/O costs [16], or possibly for"
"engendering a combination of index selection, partitioning and replication for"
both stand-alone databases [11] and parallel databases [2].
But with increasing complexity and agility of database applications and the
"introduction of modern database environments such as database as a service,"
the aforementioned tasks of database administrators are becoming more tedious
and problematic. Therefore it is desirable to design automated solutions of the
database design problem that are able to continuously monitor the incoming
queries or the changes in workload and can react readily by adapting the data-
base configuration. An online approach for physical design tuning is proposed
"in [6], that progressively chooses an optimal solution at each step through case-"
"by-case analysis of potential benefits. Similarly, [17] proposes a self-regulating"
framework for continuous online physical tuning where effective indexes are cre-
ated and deleted in response to the shifting workload. In one of the most recent
"proposals for semi-automated index tuning, WFIT [18], the authors have pro-"
posed a method based on the Work function algorithm and the feedbacks from
manual changes of configurations. To evaluate the cost of executing a query
"workload with the new indexes as well as the cost of configuration transition,"
"i.e. for profiling indexes’ benefit, most of the aforementioned online algorithms"
like WFIT exploit the what-if optimizer [7] which returns such estimated costs.
As COREIL is able to learn the estimated cost of queries gradually through
"subsequent iterations, it is applicable to a wider range of database management"
systems which may not implement what-if-optimizer or expose its interface to
the users.
"For designing and tuning online automated databases, our proposed approach"
uses the more general structure of reinforcement learning [20] that offers a rich
pool of techniques available in literature. Markov decision processes (MDPs) [13]
are one such model where each action leads to a new state and a given reward
according to a probability distribution that must be learned. On the basis of
"such a cumulative reward, these processes decide the next action to perform for"
the optimal performance. Though use of Markov decision process for modelling
"data cleaning tasks has been proposed in [4], its application in data management"
is limited because of typically huge state space and complex structures of data
"in each state (in our case, indexes). Some recent research works like [3] have"
also approached dynamic index selection based on data mining and optimiza-
"tion algorithms. But in our proposed method, COREIL, we tackle the issues of"
using reinforcement learning in database applications. Other complications like
delayed rewards obtained after a long sequence of state transitions and partial
"observability [21] of current state due to uncertainty, are also less prevalent in"
the proposed structure of COREIL.
Let R be a logical database schema. We can consider R to be the set of,its
possible database instances. Let S be the set of physical database configurations,
"of instances of R. For a given database instance, two configurations s and s′",
"may differ in the indexes, materialized views, partitions, replicas, and other",
"parameters. For a given instance, two configurations will be logically equivalent",
if they yield the same results for all queries and updates.,
The cost of changing configuration from s ∈ S to s′ ∈ S is denoted by the,
"function δ(s, s′). The function δ(s, s′) is not necessarily symmetric as the cost",
of changing configuration from s to s′ and the reverse may not be the same.,
"On the other hand, it is a non-negative function and also verifies the identity of",
indiscernibles (there is no free configuration change) and the triangle inequality,
"(it is always cheaper to do a direct configuration change). Therefore, it is",a
quasi-metric on S.,
"Let Q be a workload set, defined as a schedule of queries and updates (for",
"brevity, we refer to both as queries). Without significant loss of generality, we",
consider the schedule to be sequential and the issue of concurrency control,
"orthogonal to the current presentation. Query q is the ttht query in the schedule,",
which is executed at time t.,
The cost of executing query q ∈ Q on configuration s ∈ S is denoted by the,
"function cost(s, q). We model a query qt as a random variable, whose generating",
distribution may not be known a priori : qt is only observable at time t.,
Let s0 be the initial configuration of the database. At any time t the config-,
uration is changed from st−1 to st with the following events in order:,
1. Arrival of query qt. We call q̂t the observation of qt at time t.
"2. Choice of the configuration st ∈ S based on q̂1, q̂2, . . . , q̂t and st−1."
3. Change of configuration from st−1 to st. If no configuration change occurs at
"time t, then st = st−1."
4. Execution of query q̂t under the configuration st.
"The cost of configuration change and query execution at time t, referred as"
"the per-stage cost, is"
"C(st−1, st, q̂t) := δ(st−1, st) + cost(st, q̂t)"
We can phrase in other words the stochastic decision process of choosing the
configuration changes as a Markov decision process (MDP) [13] where states are
"database configurations, actions are configuration changes, and penalties (neg-"
ative rewards) are the per-stage cost of the action. Note that transitions from
one state to another on an action are deterministic (in contrast to the general
"framework of MDPs, there is no uncertainty associated with the new configura-"
"tion when a configuration change is decided). On the other hand, penalties are"
"both stochastic (they depend on the query, a random variable) and uncertain"
"(the cost of a query in a configuration is not known in advance, in the absence"
of a reliable cost model).
"Ideally, the problem would be to find the sequence of configurations that"
"minimizes the sum of future per-stage costs; of course, assuming an infinite"
"horizon [20], this sum is infinite. One practical way to circumvent this problem"
is to introduce a discount factor γ that gives more importance to immediate costs
"than to costs distant in the future, and to try and minimize a cumulative cost"
"defined with γ. Under Markov assumption, a sequence of configuration changes"
"is determined by a policy π : S × Q → S, which, given the current configuration"
"st−1 and a query q̂t, returns a configuration st := π(st−1, q̂t)."
We define the cost-to-go function V π for a policy π as:
V π(s) :=,E,"∑∞ st = π(st−1, q̂t), t ≥ 1t=1",γ t−1,,"C(st−1, st, q̂t)",satisfying,s 0,,= s,,(1)
where 0,<,γ,<,1 is,the discount,factor. The value,of V π(s),represents,the
expected cumulative cost for the following policy π from the current configura-,,,,,,,,,
tion s.,,,,,,,,,
Let U,be the set of all policies for a given database schema. Our problem,,,,,,,,
"can now be formally phrased as to minimize the expected cumulative cost, i.e.,",,,,,,,,,
to find an optimal policy π∗ such that π∗ := arg min,,,,,,π∈U V,π(s0).,,
4 Adaptive Performance Tuning,,,,,,,,,
4.1 Algorithm Framework,,,,,,,,,
In order,"to find the optimal policy π∗, we",,,,,"start from an arbitrary policy π,",,,
compute,an,estimation,,,of its cost-to-go,"function, and",incrementally,attempt,
to improve it using the current estimate of the cost-to-go function V for each
s ∈ S. This strategy is known as policy iteration [20] in reinforcement learning
literature.
"Assuming the probability distribution of qt is known in advance, we improve"
"π"
the cost-to-go function V tof the policy πt at iteration t using
"",πt,,,π,,
V,(s) = min,"δ(s, s′) + E [cost(s′, q)] +",γV,,t−1,(s′)
"",s′∈S,,,,,
"",,,π
We obtain the updated policy as arg min π ∈U V,t,,t(s). The algorithm terminates
when there is no change in the policy. The proof of optimality and convergence,,,
of policy iteration can be found in [12].,,,
"Unfortunately, policy iteration suffers from several problems. First, there may",,,
"not be any proper model available beforehand for the cost function cost(s, q).",,,
"Second, the curse of dimensionality [12] makes",,,the direct computation of V
"hard. Third, the probability distribution of queries is not assumed to be known",,,
"a priori, making it impossible to compute the expected cost of query execution",,,
"E [cost(s′, q)].",,,
Algorithm 1. Algorithm Framework
1: Initialization: an arbitrary policy π0 and a cost model C0
2: Repeat till convergence
"πt−13: V ← approximate using a linear projection over φ(s)"
"4: πt−15: Ct−1 ← approxim(ate using a linear p)rojection over η(s, q̂t)← πt arg min Ct−1s∈S′ + γV (s)"
6: End
The basic framework of our algorithm is shown in Algorithm1. Initial pol-
icy π0 and cost model C0 can be intialized arbitrarily or using some intelligent
"heuristics. In line 5 of Algorithm1, we have tried to overcome the issues at the"
root of the curse of dimensionality by juxtaposing the original problem with
"approximated per-stage cost and cost-to-go function. Firstly, we map a configu-"
"ration to a vector of associated feature φ(s). Then, we approximate the cost-to-"
go function by a linear model θT φ(s) with parameter θ. It is extracted from a
reduced subspace S′ of configuration space S that makes the search for optimal
"policy computationally cheaper. Finally, we learn the per-stage cost C(s, s′, q̂)"
"by a linear model ζT η(s, q̂) with parameter ζ. This method does not need any"
"prior knowledge of the cost model, rather it learns the model iteratively. Thus,"
we have resolved shortcomings of policy iteration and the need of predefined cost
model for the performance tuning problem in our algorithm. These methods are
depicted and analyzed in the following sections.
4.2 Reducing the Search Space
"To reduce the size of search space in line 5 of c 1, we filter the configurations"
that satisfy certain necessary conditions deduced from an optimal policy.
Proposition 1. Let s be any configuration and q̂ be any observed query. Let π∗
"be an optimal policy. If π∗(s, q̂) = s′, then cost(s, q̂) − cost(s′, q̂) ≥ 0. Further-"
"more, if δ(s, s′) > 0, i.e., if the configurations certainly change after query, then"
"cost(s, q̂) − cost(s′, q̂) > 0."
Proof.,"Since π∗(s, q̂) = s′, we have",
"","δ(s, s′) + cost(s′, q̂) + γV (s′)",
"","≤ cost(s, q̂) + γV ([s)",]
"","= cost(s, q̂) + γ min (δ(s, s′′E ) + cost(s′′, q̂) + γV (s′′))",
"",s′′,
"","≤ cost(s, q̂) + γδ(s, s′) + γV (s′),",
"where the second inequality is obtained by exploiting triangle inequality δ(s, s′′) ≤"
"δ(s, s′) + δ(s′, s′′), as δ is a quasi-metric on S."
This infers that
"cost(s, q̂) − cost(s′, q̂) ≥ (1 − γ)δ(s, s′) ≥ 0."
The assertion follows.,,,,,
"By Proposition 1, if π∗",is an optimal policy and s′,"= π∗(s, q̂) =",,"s,",then
"cost(s, q̂) > cost(s′, q̂). Thus, we can define a reduced subspace as",,,,,
"S s,q̂ = {s",′,,"∈ S | cost(s, q̂) > cost(s′, q̂)}."
"",,,,π,,,
"πt =",arg min,"δ(st−1, s) + cost(s, q̂t) +",γV,t−1,,(s),.
"","s∈Sst−1,q̂t",,,,,,
"Next, we design an algorithm that converges to an optimal policy through"
"searching in the reduced set Ss,q̂."
4.3 Modified Policy Iteration with Cost Model Learning
We calculate the optimal policy using the least square policy iteration (LSPI) [9].
"If for any policy π, there exists a vector θ such that we can approximate V π(s) ="
"θT φ(s) for any configuration s, then LSPI converges to the optimal policy. This"
mathematical guarantee makes LSPI an useful tool to solve the MDP as defined
in Sect. 3. But the LSPI algorithm needs a predefined cost model to update the
policy and evaluate the cost-to-go function. It is always not obvious that any
"form of cost model would be available and as mentioned in Sect. 1, pre-defined"
cost models may be critically wrong. This motivates us to develop another form
"of the algorithm, where the cost model can be equivalently obtained through"
learning.
Algorithm 2. Recursive least squares estimation.,,,
"1: procedure RLSE(̂ , B t t−1, ζt−1, ηt)",,,
2: γt ← 1 + (ηt)T B t−1ηt,,,
3: B t ← B t−1 − γ1 t (B t−1 ηt(ηt)T B t−1),,,
4: ζt ← ζt−1 − γt1 B t−1ηt̂t,,,
"5: return Bt, ζt.",,,
6: end procedure,,,
Algorithm 3. Least squares policy iteration with RLSE.,,,
1: Initialize the configuration s0.,,,
2: Initialize θ0 = θ = 0 and B0 = I.,,,
0,,,
3: Initialize ζ0 = 0 and B = I.,,,
"4: for t=1,2,3,. . . do",,,
5: Let q̂t be the just received query.,,,
"6: st ← arg min (ζ t−1 ) T η(st−1, q(st−1, s)) + (ζ t−1 ) T","Tη(s, q̂t) + γθ",,φ(s)
"s∈Sst−1,q̂t",,,
7: Change the configuration to st.,,,
8: Execute query q̂t.,,,
"9: Ĉt ← δ(st−1, st) + cost(st, q̂t).",,,
"10: ̂t ← (ζt−1)T η(st−1, q̂t) − cost(st−1, q̂t)",,,
11: Bt ← Bt − 1 − Bt−1φ(s t − 1 )(φ(s t − 1 )−γφ(s ))T Bt−1t,,,
1+(φ(s t − 1 )−γφ(s ))T Bt−1 t φ(st−1).,,,
"(Ĉt−(φ(s )−γφ(s )T θt−1)Bt−1φ(s )",,,
12: θt ← θt−1 + 1+(φ(st−1)−γφ(s ))T Bt−1t t−1 t φ(st−1)t−1 .,,,
t t−1,,,
"13: (B , ζt) ← RLSE(̂t, B , ζt−1, ηt)",,,
14: if (θt) converges then,,,
15: θ ← θt .,,,
16: end if,,,
17: end for,,,
"Assume that there exists a feature mapping η such that cost(s, q) ≈ ζT η(s, q)"
for some vector ζ. Changing the configuration from s to s′ can be considered as
"executing a special query q(s, s′). Therefore we approximate"
"δ(s, s′) = cost(s, q(s, s′)) ≈ ζT η(s, q(s, s′))."
The vector ζ can be updated iteratively using the well-known recursive least
"squares estimation (RLSE) [22] as shown in Algorithm2, where ηt = η(st−1, q̂t)"
"and 	̂t = (ζt−1)T ηt − cost(st−1, q̂t) is the prediction error. Combining RLSE"
"with LSPI, we get our cost-model oblivious algorithm as shown in Algorithm3."
"In Algorithm3, the vector θ determines the current policy. We can make"
"decision by solving the equation in line 6. The values of δ(st−1, s) and cost(s, q̂t)"
are obtained from the cost model. The vector θt is used to approximate the
"cost-to-go function following the current policy. If θt converges, then we update"
the current policy (line 14–16).
"To check the efficiency and effectiveness of this algorithm, instead of using"
any heuristics we have initialzed policy π0 as initial configuration s0 and the
cost-model C0 as 0 shown in the lines 1–3 of Algorithm 3.
5 Case Study: Index Tuning
"In this section, we present COREIL, an algorithm for tuning the configurations"
differing in their secondary indexes and handling the configuration changes corre-
"sponding to the creation and deletion of indexes, which instantiates Algorithm3."
5.1 Reducing the Search Space
Let I be the set of indexes that can be created. Each configuration s ∈ S is an
"element of the power set 2I . For example, 7 attributes in a schema of R yield a"
total of 13699 indexes and a total of 213699 possible configurations. Such a large
search space invalidates a naive brute-force search for the optimal policy.
"For any query q̂, let r(q̂) be a function that returns a set of recommended"
"indexes. This function may be already provided by the database system (e.g.,"
"as with IBM DB2), or it can be implemented externally [1]. Let d(q̂) ⊆ I be the"
"set of indexes being modified (update, insertion or deletion) by q̂. We can define"
the reduced search space as
"Ss,q̂ = {s′ ∈ S | (s − d(q̂)) ⊆ s′ ⊆ (s ∪ r(q̂))}. (4)"
Deleting indexes in d(q̂) will reduce the index maintenance overhead and creating
indexes in r(q) will reduce the query execution cost. Note that the definition of
"Ss,q̂ here is a subset of the one defined in Sect. 4.2 which deals with the general"
configurations.
"Note that for tree-structured indexes (e.g., B+-tree), we could further con-"
"sider the prefix closure of indexes for optimization. For any configuration s ∈ 2I ,"
define the prefix closure of s as
"〈s〉 = {i ∈ I | i is a prefix of an index j for some j ∈ s}. (5)"
"Thus in Eq. (4), we use 〈r(q̂)〈 to replace r(q̂) for better approximation. The"
intuition is that in case of i ∈/ s but i ⊆ 〈s〉 we can leverage the prefix index to
answer the query.
5.2 Defining the Feature Mapping φ
"Let V be the cost-to-go function following a policy. As mentioned earlier,"
Algorithm 3 relies on a proper feature mapping φ that approximates the cost-
to-go function as V (s) ≈ θT φ(s) for some vector θ. The challenge lies in how to
"define φ under the scenario of index tuning. In COREIL, we define it as"
"","1,",if s′ ⊆ s
"φs′(s) :=","−1,",otherwise.
"for each s, s′ ∈ S. Let φ = (φs′)s′∈S . Note that φ∅ is an intercept term since"
"φ∅(s) = 1 for all s ∈ S. The following proposition shows the effectiveness of φ"
for capturing the values of the cost-to-go function V .
Proposition 2. There exists a unique θ = (θs′)s′∈S which approximates the
value function as ∑
V (s) = θ ′(s) = θTs ′φ s φ(s). (6)
s′∈S
"Proof. Suppose S = {s1, s2, . . . , s|S|}. Note that we use superscripts to denote"
the ordering of elements in S.
Let V = (V (s))Ts ∈ S and M be a |S| × |S| matrix such that
"Mi,j = φ sj (s ).i"
Let θ be a |S|-dimension column vector such that Mθ = V . If M is invertible
then θ = M−1V and thus Eq. (6) holds.
We now show that M is invertible. Let ψ be a |S| × |S| matrix such that
"ψi,j = Mi,j + 1."
We claim that ψ is invertible and its inverse is the matrix τ such that
"τi,j = (−1)|s i |−|s j|ψi,j ."
"To see this, consider ∑"
i k
"(τψ)i,j = (−1)|s |−|s |ψi,kψk,j"
1≤k∑≤|S|
i
"= (−1)|s |−|sk|."
sj⊆sk⊆si
"Therefore (τψ)i,j = 1 if and only if i = j. By the Sherman-Morrison formula,"
M is also invertible.
"However, for any configuration s, θ(s) is a |2I |-dimensional vector. To reduce"
"t∑he dimensionality, the cost-to-go function can be approximated by V (s) ≈"
"θs′φs′s ′∈ S, s | ′|≤ N (s) for some integer N . Here we assume that the collaborative"
benefit among indexes could be negligible if the number of indexes exceeds N .
"In particular when N = 1, we have ∑"
V (s) ≈ θ0 + θiφi(s). (7)
i∈I
where we ignore all the collaborative benefits among indexes in a configuration.
This is reasonable since any index in a database management system is often of
"individual contribution for answering queries [15]. Therefore, we derive φ from"
"Eq. (7) as φ(s) = (1, (φi(s))Ti∈I)T . By using this feature mapping φ, COREIL"
approximates the cost-to-go function V (s) ≈ θT φ(s) for some vector θ.
5.3 Defining the Feature Mapping η
A good feature mapping for approximating functions δ and cost must take into
account both the benefit from the current configuration and the maintenance
overhead of the configuration.
To capture the difference between the index set recommended by the database
"system and that of the current configuration, we define a function β(s, q̂) ="
"(1, (β (s, q̂))T i i∈I) T, where"
"⎨0,",i ∈/ r(q̂)
"βi(s, q̂) := ⎪⎩1, −1,",i ∈ r(q̂) and i ∈ si ∈ r(q̂) and i ∈/ s.
If the,execution of query,q̂,cannot benefit,from,index,i,"then βi(s, q̂)",always
"equals zero; otherwise, βi(s, q̂) equals 1 or –1 depending on whether s contains i",,,,,,,,
"or not. For tree-structured indexes, we could further consider the prefix closure",,,,,,,,
of indexes as defined in Eq. (5) for optimization.,,,,,,,,
"On the other hand, to capture whether a query (update, insertion or deletion)",,,,,,,,
"modifies any index in the current configuration, we define a function α(s, q̂) =",,,,,,,,
"(αi(s, q̂))i∈I where",,,,,,,,
"","1,",if i ∈ s and q̂ modify i
"αi(s, q̂) =",,
"","0,",otherwise.
"Note that if q̂ is a selection query, α trivially returns 0."
"By combining β and α, we get the feature mapping η = (βT ,αT )T used in"
COREIL. It can be used to approximate the functions δ and cost as described
in Sect. 4.3.
6 Performance Evaluation,
"In this section, we present an empirical evaluation of COREIL. We implement",
a prototype of COREIL in Java and compare its performance with that of the,
state-of-the-art WFIT Algorithm [18]. WFIT,is based on the Work Function
Algorithm [5]. To determine the change,"of configuration, it considers all the"
queries seen so far and solves a deterministic problem towards minimizing the,
total processing cost.,
6.1 Experimental Setup,
We conduct all the experiments on a server running IBM DB2 10.5. The server,
is equipped with Intel i7-2600 Quad-Core @ 3.40 GHz and 4 GB RAM. We mea-,
"sure wall-clock times for execution of all components. Specially, for execution of",
"workload queries or index creating/dropping, we measure the response time of",
"processing corresponding SQL statement in DB2. Additionally, WFIT uses the",
"what-if optimizer of DB2 to evaluate the cost. In this setup, each query is exe-",
cuted only once and all the queries were generated from one execution history.,
The scale factor (SF) used here is 2.,
Fig. 1. Evolution of the efficiency (total time per query) of the two systems from the
beginning of the workload (smoothed by averaging over a moving window of size 20)
6.2 Dataset and Workload,
The dataset and workload is conforming to the TPC-C specification [14] and,
generated by the OLTP-Bench tool,[8]. The 5 types of transactions in TPC-
"C are distributed as NewOrder 45 %, Payment 43 4 transactions are associated",
with 3 ∼ 5 SQL statements (query/update). Note that [18] additionally uses the,
"dataset NREF in its experiments. However, this dataset and workload are not",
publicly available.,
6.3 Efficiency,
Figure 1 shows the total cost of processing TPC-C queries for online index tuning,
of COREIL and WFIT. Total cost consists of the overhead of corresponding tun-,
"ing algorithm, cost of configuration change and that of query execution. Results",
"show that, after convergence, COREIL has lower processing cost most of the",
"time. But COREIL converges slower than WFIT, which is expected since it does",
not rely on the what-if optimizer to guide the index creations. With respect to the,
"whole execution set, the average processing cost of COREIL (451 ms) is compet-",
"itive to that of WFIT (452 ms). However, if we calculate the average processing",
"cost of the 500th query forwards, the average performance of COREIL (357 ms)",
"outperforms that of WFIT (423 ms). To obtain further insight from these data,",
"we study the distribution of the processing time per query, as shown in Fig. 2.",
"As can be seen, although COREIL exhibits larger variance in the processing cost,",
its median is significantly lower that that of WFIT. All these results confirms,
that COREIL has better efficiency than WFIT under a long term execution.,
Fig. 2. Box chart of the efficiency (total time per query) of the two systems. We show
"in both cases the 9th and 91th percentiles (whiskers), first and third quartiles (box)"
and median (horizontal rule).
Figures 3 and 4 show analysis of the overhead of corresponding tuning algo-
rithm and cost of configuration change respectively. By comparing Fig. 1 with
"Fig. 3, we can see that the overhead of the tuning algorithm dominates the total"
cost and the overhead of COREIL is significantly lower than that of WFIT.
"In addition, WFIT tends to make costlier configuration changes than COREIL,"
which is reflected in a higher time for configuration change. This would be dis-
cussed further in the micro-analysis. Note that both methods converge rather
quickly and no configuration change happens beyond the 700th query.
6.4 Effectiveness
"To verify the effectiveness of indexes created by the tuning algorithms, we extract"
the cost of query execution from the total cost. Figure 5 (note the logarithmic
y-axis) indicates that the set of indexes created by COREIL shows competitive
"effectiveness with that created by WFIT, though WFIT is more effective in gen-"
"eral and exhibits less variance after convergence. Again, this is to be expected"
since COREIL does not have access to any cost model for the queries. As previ-
"ously noted, the total running time is lower for COREIL than WFIT, as overhead"
rather than query execution dominates running time for both systems.
We have also performed a micro-analysis to check whether the indexes created
by the algorithms are reasonable. We observe that WFIT creates more indexes
"with longer compound attributes, whereas COREIL is more parsimonious in"
"creating indexes. For instance, WFIT creates a 14-attribute index as shown"
below.
"[S_W_ID, S_I_ID, S_DIST_10, S_DIST_09, S_DIST_08, S_DIST_07,"
Fig. 3. Evolution of the overhead (time of the optimization itself) of the two systems
from the beginning of the workload (smoothed by averaging over a moving window of
size 20)
Fig. 4. Evolution,of,the,time,taken,by,configuration,change,(index,creation,and
destruction) of the two systems from the beginning of the workload; no configuration,,,,,,,,,,
change happens past query #1000,,,,,,,,,,
Fig. 5. Evolution of the effectiveness (query execution time in the DBMS alone) of the
two systems from the beginning of the workload (smoothed by averaging over a moving
window of size 20); logarithmic y-axis
"S_DIST_06, S_DIST_05, S_DIST_04, S_DIST_03, S_DIST_02,"
"S_DIST_01, S_DATA, S_QUANTITY]"
The reason of WFIT creating such a complex index is probably due to multiple
queries with the following pattern.
"SELECT S_QUANTITY, S_DATA, S_DIST_01, S_DIST_02, S_DIST_03,"
"S_DIST_04, S_DIST_05, S_DIST_06, S_DIST_07, S_DIST_08,"
"S_DIST_09, S_DIST_10"
FROM STOCK
WHERE S_I_ID = 69082 AND S_W_ID = 1;
"In contrast, COREIL tends to create shorter compound-attribute indexes."
"For example, COREIL created an index [S I ID, S W ID] which is definitely"
beneficial to answer the query above and is competitive in performance compared
with the one created by WFIT.
7 Conclusion
We have presented a cost-model oblivious solution to the problem of performance
tuning. We have first formalized this problem as a Markov decision process.
"We have devised and presented a solution, which addresses the curse of dimen-"
sionality. We have instantiated the problem to the case of index tuning and
implemented the COREIL algorithm to solve it. Experiments show competi-
"tive performance with respect to the state-of-the-art WFIT algorithm, despite"
COREIL being cost-model oblivious.
Now that we have validated the possibility for cost-model oblivious database
"tuning, we intend in future work to study the trade-off for COREIL between"
efficiency and effectiveness in the case of index tuning. To show universality and
"robustness of COREIL, we are planning to run further tests on other datasets"
"like TPC-E, TPC-H and benchmark for online index tuning. To find out its"
"sensitivity on setup, we want to experiment with varying scale factors and and"
"other parameters. Furthermore, we want to extend our approach to other aspects"
"of database configuration, including partitioning and replication. This is not"
"straightforward, as the solution will require heuristics that help curb the combi-"
natorial explosion of the configuration space as well as may need some intelligent
initialization technique.
Acknowledgement. This research is funded by the National Research Foundation
Singapore under its Campus for Research Excellence and Technological Enterprise
"(CREATE) programme with the SP2 project of the Energy and Environmental Sus-"
tainability Solutions for Megacities - E2S2 programme.
