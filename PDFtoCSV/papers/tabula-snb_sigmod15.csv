"tional table, for instance as a table where every row contains"
"an edge, and the start and end vertex of every edge are a"
"foreign key reference (in SQL terms). However, what makes"
a data management problem a graph problem is that the
data analysis is not only about the values of the data items
"in such a table, but about the connection patterns between"
the various pieces. SQL-based systems were not originally
designed for this – though systems have implemented diverse
extensions for navigational and recursive query execution.
"In recent years, the database industry has seen a prolifer-"
ation of new graph-oriented data management technologies.
"Roughly speaking, there are four families of approaches."
"One are pure graph database systems, such as Neo4j, Spark-"
"see and Titan, which elevate graphs to first class citizens in"
"their data model (“property graphs”), query languages, and"
APIs. These systems often provide specific features such as
"breadth-first search and shortest path algorithms, but also"
"allow to insert, delete and modify data using transactional"
semantics. A second variant are systems intended to manage
"semantic web data conforming to the RDF data model, such"
as Virtuoso or OWLIM. Although RDF systems emphasize
"usage in semantic applications (e.g. data integration), RDF"
"is a graph data model, which makes SPARQL the only well-"
defined standard query language for graph data. A third
kind of new system targets the need to compute certain
"complex graph algorithms, that are normally not expressed"
"in high-level query languages, such as Community Finding,"
"Clustering and PageRank, on huge graphs that may not fit"
"the memory of a single machine, by making use of cluster"
"computing. Example systems are GraphLab, Stratosphere"
"and Giraph, though this area is still heavily in motion and"
"does not yet have much industrial installed base. Finally, re-"
"cursive SQL, albeit not very elegant, is expressive enough to"
construct a large class of graph queries (variable length path
"queries, pattern matching, etc.). One of the possibilities"
"(exemplified by Virtuoso RDBMS) is to introduce vendor-"
"specific extensions to SQL, which are basically shortcuts for"
recursive SQL subqueries to run specific graph algorithms
inside SQL queries (such as shortest paths).
The Linked Data Benchmark Council (LDBC) is now two
years underway and has gathered strong industrial partici-
"pation for its mission to establish benchmarks, and bench-"
marking practices for evaluating graph data management
systems. The LDBC introduced a new choke-point driven
"methodology for developing benchmark workloads, which"
combines user input with input from expert systems archi-
"tects, which we outline. This paper describes the LDBC"
"Social Network Benchmark (SNB), and presents database"
benchmarking innovation in terms of graph query function-
"ality tested, correlated graph generation techniques, as well"
as a scalable benchmark driver on a workload with complex
graph dependencies. SNB has three query workloads under
"development: Interactive, Business Intelligence, and Graph"
Algorithms. We describe the SNB Interactive Workload in
"detail and illustrate the workload with some early results,"
as well as the goals for the two other workloads.
1. INTRODUCTION
Managing and analyzing graph-shaped data is an increas-
"ingly important use case for many organizations, in for in-"
"stance marketing, fraud detection, logistics, pharma, health-"
care but also digital forensics and security. People have
"been trying to use existing technologies, such as relational"
database systems for graph data management problems. It
is perfectly possible to represent and store a graph in a rela-
The Linked Data Benchmark Council1 (LDBC) is an in-
"dependent authority responsible for specifying benchmarks,"
benchmarking procedures and verifying/publishing bench-
mark results. Benchmarks on the one hand allow to quan-
"titatively compare di↵erent technological solutions, helping"
IT users to make more objective choices for their software
"architectures. On the other hand, an important second goal"
for LDBC is to stimulate technological progress among com-
peting systems and thereby accelerate the maturing of the
new software market of graph data management systems.
"This paper describes the Social Network Benchmark (SNB),"
"the first LDBC benchmark, which models a social network"
akin to Facebook. The dataset consists of persons and a
friendship network that connects them; whereas the major-
ity of the data is in the messages that these persons post in
discussion trees on their forums. While SNB goes through
lengths to make its generated data more realistic than previ-
"ous synthetic approaches, it should not be understood as an"
attempt to fully model Facebook – its ambition is to be as re-
alistic as necessary for the benchmark queries to exhibit the
desired e↵ects – nor does the choice for social network data
as the scenario for SNB imply that LDBC sees social net-
work companies as the primary consumers of its benchmarks
"– typically these internet-scale companies do not work with"
standard data management software and rather roll their
"own. Rather, the SNB scenario is chosen because it is an"
"appealing graph-centric use case, and in fact social network"
analysis on data that contains excerpts of social networks is
a very common marketing activity nowadays.
There are in fact three SNB benchmarks on one common
"dataset, since SNB has three di↵erent workloads. Each work-"
load produces a single metric for performance at the given
scale and a price/performance metric at the scale and can
be considered a separate benchmark. The full disclosure
further breaks down the composition of the metric into its
"constituent parts, e.g. single query execution times."
eS eacN t vB in-This workload consists of a set of rel-rtI .
"atively complex read-only queries, that touch a significant"
"amount of data, often the two-step friendship neighborhood"
and associated messages. Still these queries typically start
at a single point and the query complexity is sublinear to the
dataset size. Associated with the complex read-only queries
"are simple read-only queries, which typically only lookup"
one entity (e.g. a person). Concurrent with these read-only
"queries is an insert workload, under at least read commit-"
ted transaction semantics. All data generated by the SNB
"data generator is timestamped, and a standard scale fac-"
tor covers three years. Of this 32 months are bulkloaded at
"benchmark start, whereas the data from the last 4 months"
is added using individual DML statements.
BS . This workload consists of a set of queries thatN IB-
access a large percentage of all entities in the dataset (the
"“fact tables”), and groups these in various dimensions. In"
"this sense, the workload has similarities with existing re-"
lational Business Intelligence benchmarks like TPC-H and
TPC-DS; the distinguishing factor is the presence of graph
traversal predicates and recursion. Whereas the SNB In-
"teractive workload has been fully developed, the SNB BI"
"workload is a working draft, and the concurrent bulk-load"
workload has not yet been specified.
"This wS ogN .msB A uc i n,- l",to orkload is under consthr ti,r
but is planned to consist of a handful of often-used graph,,
"analysis algorithms, including PageRank, Community De-",,
"tection, Clustering and Breadth First Search.",,While we
foresee that the two other SNB workloads can be used to,,
"compare graph database systems, RDF stores, but also SQL",,
stores or even noSQL systems; the SNB-Algorithms work-,,
load primary targets graph programming systems or even,,
general purpose cluster computing environments like MapRe-,,
"duce. It may, however, be possible to implement graph algo-",,
"rithms as iterative queries, e.g. keeping state in temporary",,
"tables, hence it is possible that other kinds of systems may",,
also implement it.,,
Given that graph queries and graph algorithm complexity,,
"is heavily influenced by the complex structure of the graph,",,
we specifically aim to run all three benchmarks on the same,,
"dataset. In the process of benchmark definition, the dataset",,
"generator is being tuned such that the graph, e.g. contains",,
"communities, and clusters comparable to clusters and com-",,
munities found on real data. These graph properties cause,,
"the SNB-Algorithms workload to produce “sensible” results,",,
but are also likely to a↵ect the behavior of queries in SNB-,,
"Interactive and SNB-BI. Similarly, the graph degree and val-",,
ue/structure correlation (e.g.,,people having names typical
for a country) that a↵ect query outcomes in SNB-Interactive,,
and BI may also implicitly a↵ect the complexity of SNB-,,
"Algorithms. As such, having three diverse workloads on the",,
same dataset is thought to make the behavior of all work-,,
"loads more realistic, even if we currently would not under-",,
stand or foresee how complex graph patterns a↵ect all graph,,
management tasks.,,
"This paper focuses on SNB-Interactive, since this work-",,
load is complete.,The goal of SNB-Interactive is to test,
graph data management systems that combine transactional,,
update with query capabilities. A well-known graph database,,
"system that o↵ers this is Neo4j, but SNB-Interactive is for-",,
"mulated such that many systems can participate, as long",,
a they support transactional updates allowing simultaneous,,
queries.,"The query workload focus on interactivity, with",
the intention of sub-second response times and query pat-,,
terns that start typically at a single graph node and visit,,
only a small portion of the entire graph. One could hence,,
"position it as OLTP, even though the query complexity is",,
much higher than TPC-C and does include graph tasks such,,
as traversals and restricted shortest paths.,,The rationale
for this focus stems from LDBC research among its ven-,,
dor members and the LDBC Technical User Community of,,
database users. This identified that many interactive graph,,
applications currently rely on key-value data management,,
"systems without strong consistency, where query predicates",,
that are more complex than a key-lookup are answered us-,,
ing o✏ine pre-computed data.,,This staleness and lack of
consistency both impact the user experience and compli-,,
"cate application development, hence LDBC hopes that SNB-",,
Interactive will lead to the maturing of transactional graph,,
data management systems that can improve the user expe-,,
rience and ease application development.,,
The main contributions by the LDBC work on SNB are,,
the following:,,
es ec aa ol ra or h sb hl te,nc pr . The SNB graage ralated r p h,g
been shown to be much more realistic than previous syn-,,
"thetic data generators [13], for which reason it was already",,
chosen to be the base of the 2014 SIGMOD programming,,
contest. The graph generator is further notable because it
"realizes well-known power laws, uses skewed value distri-"
"butions, but also introduces plausible correlations between"
property values and graph structures.
nds dbnc op-eho sigtk ea ei . The SNB-Interactive query
workload has been carefully designed according to so-called
choke-point analysis that identifies important technical chal-
lenges to evaluate in a workload. This analysis requires both
user input2 as well as expert input from database systems ar-
"chitects. In defining the SNB-Interactive, LDBC has worked"
"with the core architects of Neo4j, RDF-3X, Virtuoso, Spark-"
"see, MonetDB, Vectorwise and HyPer."
nand syne ip he tydnoce c r zo The SNB query driverin .
solves the di cult task of generating a highly parallel work-
"load to achieve high throughput, on a datasets that by its"
complex connected component structure is impossible to
partition. This could easily lead to extreme overhead in
the query driver due to synchronization between concurrent
client threads and processes – the SNB driver enables op-
timizations that strongly reduce the need for such synchro-
nization by identifying sequential and window-based execu-
tion modes for parts of the workload.
cp ra Since the SNB dataset is such aerma e itt nou r a.
"complex graph, with value/structure correlations a↵ecting"
"queries over the friends graph and message discussion trees,"
with most distributions being either skewed (typically using
"the exponential distribution) or power-laws, finding good"
query parameters is non-trivial. If uniformly chosen val-
"ues would serve as parameters, the complexity of any query"
template would vary enormously between the parameters –
an undesirable phenomenon for the understandability of a
benchmark. The SNB therefore introduced a new bench-
"marking concept, namely Parameter Curation [6] that per-"
forms a data mining step during data generation to find
substitution parameters with equivalent behavior.
2. INNOVATIVE GRAPH GENERATOR
The LDBC SNB data generator (DATAGEN) evolved from
the S3G2 generator [10] and simulates the user’s activity in a
social network during a period of time. Its schema has 11 en-
"tities connected by 20 relations, with attributes of di↵erent"
"types and values, making for a rich benchmark dataset. The"
"main entities are: Persons, Tags, Forums, Messages (Posts,"
"Comments and Photos), Likes, Organizations, and Places."
A detailed description of the schema is found at [11].
The dataset forms a graph that is a fully connected com-
ponent of persons over their friendship relationships. Each
person has a few forums under which the messages form
large discussion trees. The messages are further connected
to posts by authorship but also likes. These data elements
scale linearly with the amount of friendships (people having
more friends are likely more active and post more messages).
Organization and Place information are more dimension-like
and do not scale with the amount of persons or time. Time
is an implicit dimension (there is no separate time entity)
but is present in many timestamp attributes.
"e(person.location, person.firstName",s( mt ap cy ai )l n,,,,
person.gender) person.interests,r( )sp pu al ro a,,tti,,
person.location person.lastName,aci( )st ey p aml n,,,,
person.university,un( )n te ra r iyb sesiev,i,,,
person.company,( tri nn y)cou,,,,
person.languages,)ns( pyrocio ek nn tu,,,,
person.language person.forum.post.language,sak( es)p,,,,
person.interests person.forum.post,).t ( nopi ic,,,,
laedp lo t. is topic ep siost.text,( )DBp a net cr i,,,,
Bpost.comment i. e (t tipDx,lt e cd eia n sea r l,,,,)
mpap rson.employer e person.email te(,"i@c )no uy @, sn riyv",,,,
post.photoLoc )at oion p ntios .lo et ht aca stion.l m(atitude,c l oc a,,,,
cp (o nimst. o cl oacation. lo ang,)eitu hde t os l,t,,,
person.birthDate person.c )r (e a tedDate,,,,>,
person.createdDate person.forum.message.createdDa,)(te,,,>,
")person.f r (o um.createdDate",,,,>,
forum.createdDate post.photoTim (e,,,,>),
"(forum.post.create",)dDate,,,>,
forum.groupme )mbership.,(j oi nedD ate,,,>,
post. )createdDate post.comment.createdDate,,,(>,,
2.1 Correlated Attribute Values,
An important novelty in DATAGEN is the ability to pro-,
"duce a highly correlated social network graph, in which at-",
tribute values are correlated among themselves and also in-,
fluence the connection patterns in the social graph.,Such
correlations clearly occur in real graphs and influence the,
complexity of algorithms operating on the graph.,
A full list of attribute correlations is given in Table 1.,
"For instance, the top row in the table states that the place",
where a person was born and gender influence the first name,
"distribution. An example is shown in Table 2, which shows",
the top-10 most occurring first names for people from Ger-,
many vs China. The actual set of attribute values is taken,
"from DBpedia, which also is used as a source for many other",
"attributes. Similarly, the location where a person lives in-",
"fluences his/her interests (a set of tags), which in turn influ-",
"ences the topic of the discussions (s)he opens (i.e., Posts),",
which finally also influences the text of the messages in the,
discussion. This is implemented by using the text taken,
from DBpedia pages closely related to a topic as the text,
used in the discussion (original post and comments on it).,
"Person location also influences last name, university, com-",
"pany and languages. This influence is not full, there are",
"Germans with Chinese names, but these are infrequent. In",
"fact, the shape of the attribute value distributions is equal",
"(and skewed), but the order of the values from the value",
"dictionaries used in the distribution, changes depending on",
the correlation parameters (e.g. location).,
The probability for generating a connection during this stage
drops from very low at window boundary to zero outside
it (since the generator is not even capable of generating a
friendship to data dropped from its window). All this makes
the complex task of generating correlated friendship edges
"scalable, as it now only depends on parallel sorting and se-"
quential processing with limited memory. We note that one
dimension may have the form of multiple single-dimensional
values bitwise appended. In the particular case of the stud-
"ied location, these are the Z-order location of the university’s"
"city (bits 31-24), the university ID (bits 23-12), and the stud-"
ied year (bits 11-0). This is exemplified at Figure 1 where
we show a sliding window along the first correlation dimen-
"sion (i.e., studied location). As shown in this figure, those"
persons closer to person P2 (the person generating friends
"for) according to the first dimension (e.g., P41, P6) have a"
higher probability to be friends of P2.
The correlations in the friends graph also propagate to
the messages. A person location influences on the one hand
"interests and studied location, so one gets many more like-"
minded or local friends. These persons typically have many
"more common interests (tags), which become the topic of"
posts and comment messages.
The number of friendship edges generated per person (friend-
ship degree) is skewed [4]. DATAGEN discretizes the power
"law distribution given by Facebook graph [14], but scales"
this according to the size of the network. Because in smaller
"networks, the amount of “real” friends that is a member and"
"to which one can connect is lower, we adjust the mean aver-"
"age degree logarithmically in terms of person membership,"
such that it becomes (somewhat) lower for smaller networks.
A target average degree of the friendship graph is chosen us-
"log n. .ing the following formula: avg degree = n0 512 0 028· ( ),"
"where n is the number of persons in the graph. That is,"
when the size of the SNB dataset would be that of Facebook
"(i.e. 700M persons) the average friendship degree would be"
"around 200. Then, each person is first assigned to a per-"
"centile p in the Facebook’s degree distribution and second,"
a target degree uniformly distributed between the minimum
and the maximum degrees at percentile p. Figure 2(b) shows
"the maximum degree per percentile of the Facebook graph,"
"used in DATAGEN. Finally, the person’s target degree is"
scaled by multiplying it by a factor resulting from divid-
ing avg degree by the average degree of the real Facebook
graph. Figure 3(a) shows the friendship degree distribution
"for SF=10. Finally, given a person, the number of friendship"
edges for each correlation dimension is distributed as follows:
"45%, 45% and 10% out of the target degree, for the first, the"
"second and the third correlation dimension, respectively."
2.2 Time Correlation and Spiking Trends
Almost all entities in the SNB dataset have timestamp
"attributes, since time is an important phenomenon in social"
networks. The latter correlation rules in Table 1 are related
"to time, and ensure that events in the social network follow"
"a logical order: e.g., people can post a comment only after"
"becoming a friend with someone, and that can only happen"
after both persons joined the network.
"The volume of person activity in a real social network, i.e.,"
"number of messages created per unit of time, is not uniform,"
"but driven by real world events such as elections, natural dis-"
asters and sport competitions. Whenever an important real
"world event occurs, the amount of people and messages talk-"
ing about that topic spikes – especially from those persons
interested in that topic. We introduced this in DATAGEN
"by simulating events related to certain tags, around which"
the frequency of posts by persons interested in that tag is sig-
nificantly higher (the topic is “trending”). Figure 2(a) shows
the density of posts over time with and without event-driven
"post generation, for SF=10. When event driven post gen-"
"eration is enabled, the density is not uniform but spikes of"
"di↵erent magnitude appear, which correspond to events of"
di↵erent levels of importance. The activity volume around
an event is implemented as proposed in [7].
2.3 Structure Correlation: Friendships
The “Homophily Principle” [8] states that similar people
have a higher probability to be connected. This is modeled
by DATAGEN by making the probability that people are
connected dependent on their characteristics (attributes).
This is implemented by a multi-stage edge generation pro-
acc noimiess over two deo r sr l t ei no n s : (i) places where
people studied and (ii) interests of persons.
"In other words, people that are interested in a topic and/or"
"have studied in the same university at the same year, have"
"a larger probability to be friends. Furthermore, in order to"
"reproduce the inhomogeneities found in real data, a third"
dimension consisting of a random number is also used.
In each edge generation stage the persons are re-sorted
"on one dimension (first stage: study location, second: inter-"
"ests, last: random). Each worker processes a disjunct range"
"of these persons sequentially, keeping a window of the per-"
sons in memory – the entire range does not have to fit – and
picks friends from the window using a geometric probabil-
ity distribution that decreases with distance in the window.
"",,Number of entities (x 1000000)
SFs,,
"",Nodes,Edges Persons Friends Messages Forums
30,99.4,655.4 0.18 14.2 97.4 1.8
100,317.7,2154.9 0.50 46.6 312.1 5.0
300,907.6,6292.5 1.25 136.2 893.7 12.6
1000,2930.7,20704.6 3.60 447.2 2890.9 36.1
2.4 Scales & Scaling,
"DATAGEN can generate social networks of arbitrary size,",
however for the benchmarks we work with standard scale-,
"factors (SF) valued 1,3,10,30,.. as indicated in Table 3. The",
scale is determined by setting the amount of persons in the,
"network, yet the scale factor is the amount of GB of uncom-",
pressed data in comma separated value (CSV) representa-,
tion. DATAGEN can also generate RDF data in Ntriple3,
"format, which is much more verbose.",
DATAGEN is implemented on top of Hadoop to provide,
"scalability. Data generation is performed in three steps, each",
of them composed of more MapReduce jobs.,
"nap eer ogos ien n tr : In this step, the people of the so-",
"cial network are generated, including the personal informa-",
"tion, interests, universities where they studied and compa-",
nies where they worked at. Each mapper is responsible of,
generating a subset of the persons of the network.,
"tf As explained above, friendshipi nr e ihds op gi ne aen r:",
"generation is split into a succession of stages, each of them",
based on a di↵erent correlation dimension.,Each of these
stages consists of two MapReduce jobs. The first is respon-,
sible for sorting the persons by the given correlation dimen-,
sion. The second receives the sorted people and performs,
the sliding window process explained above.,
cp oe g thevr linegs ion a-tt i fy le er va s fit noi,n : this invol
rums with posts comments and likes.,This data is mostly
tree-structured and is therefore easily parallelized by the,
person who owns the forum.,Each worker needs the at-
tributes of the owner (e.g.,"interests influence post topics),"
the friend list (only friends post comments and likes) with,
the friendship creation timestamps (they only post after,
that); but otherwise the workers can operate independently.,
We have paid specific attention to making data generation,
deterministic. This means that regardless the Hadoop con-,
"figuration parameters (#node, #map and #reduce tasks)",
the generated dataset is always the same.,
"On a single 4-core machine (Intel i7-2600K@3.4GHz, 16GB",
RAM) that runs MapReduce in “pseudo-distributed” mode,
"– where each CPU core runs a mapper or reducer – one can",
generate a SF=30 in 20 minutes.,For larger scale factors
it is recommended to use a true cluster; SF=1000 can be,
generated within 2 hours with 10 such machines connected,
with Gigabit ethernet (see Figure 3(b)).,
3. DESIGN BY CHOKE POINTS,
LDBC benchmark development is driven by the notion of,
a choke point. A choke point is an aspect of query execution,
or optimization which is known to be problematical for the,
"present generation of various DBMS (relational, graph and",
RDF). Our inspiration here is the classical TPC-H bench-,
mark. Although TPC-H design was not based on explicitly,
"formulated choke points, the technical challenges imposed",
by the benchmark’s queries have guided research and de-,
velopment in the relational DBMS domain in the past two,
decades [3]. A detailed analysis of all choke points used to,
design the SNB Interactive workload is outside the scope,
"of this paper, the reader can find it in [11]. In general, the",
choke points cover the“usual” challenges of query processing,
"(e.g., subquery unnesting, complex aggregate performance,",
"detecting dependent group-by keys etc.), as well as some",
hard problems that are usually not part of synthetic bench-,
marks. Here we list a few examples of these:,
Estimating cardinality in graph traversals with data skew,
and correlations. As graph traversals are in fact repeated,
joins this comes back at a crucial open problem of query,
optimization in a slightly more severe form.,SNB queries
"stress cardinality estimation in transitive queries, such as",
"traversals of hierarchies (e.g., made by replies to posts) and",
dense graphs (paths in the friendship graph).,
Choosing the right join order and type. This problem is di-,
"rectly related to the previous one, cardinality estimation.",
"Moreover, there is an additional challenge for RDF systems",
where the plan search space grows much faster compared to,
equivalent SQL queries: SPARQL operates over triple pat-,
"terns, so table scans on multiple attributes in the relational",
domain become multiple joins in RDF.,
"(such as neighborhood lookup) have random access without"
"predictable locality, and e ciency of index lookup is very"
"di↵erent depending on the locality of keys. Also, detecting"
absence of locality should turn o↵ any locality dependent
optimizations in query processing.
Parallelism and result reuse. All SNB Interactive queries of-
fer opportunities for intra- and inter-query parallelism. Ad-
"ditionally, since most of the queries retrieve one- or two-hop"
"neighborhoods of persons in the social graph, and the Per-"
"son domain is relatively small, it might make sense to reuse"
results of such retrievals across multiple queries. This is an
example of recycling: a system would not only cache final
"query results, but also intermediate query results of a “high"
"value”, where the value is defined as a combination of partial"
"query result size, partial query evaluation cost, and observed"
frequency of the partial query in the workload.
In order to illustrate our choke point-based de-lpE mxa e.
"sign of SNB queries, we will describe technical challenges"
"behind one of the queries in the workload, Query 9. Its"
definition in English is as follows:
"Qu :e yr 9 Given a start Person, find the 20 most recent"
Posts/Comments created by that Person’s friends or friends
of friends. Only consider the Posts/Comments created be-
fore a given date.
"This query looks for paths of length two or three, starting"
"from a given Person, moving to the friends and friends of"
"friends, and ending at their created Posts/Comments. This"
"intended query plan, which the query optimizer has to de-"
"tect, is shown in Figure 4. Note that for simplicity we pro-"
vide the plan and discussion assuming a relational system.
While the specific query plan for systems supporting other
"data models will be slightly di↵erent (e.g., in SPARQL it"
"would contain joins for multiple attributes lookup), the fun-"
damental challenges are shared across all systems.
Although the join ordering in this case is fairly straight-
"forward, an important task for the query optimizer here"
"is to detect the types of joins, since they are highly sen-"
sitive to cardinalities of their inputs. The lower most join
11 takes only 120 tuples (friends of a given person) and joins
them with the entire sFtable to find the second degreedei nr
friends. This is best done by looking up these 120 tuples in
"the index on the primary key of , i.e. by performingF snri de"
"an index nested loop join. The same holds for the next 12,"
since it looks up around a thousand tuples in an index on
"primary key of . However, the inputs of the last 1nP sre o 3"
"are too large, and the corresponding index is not available"
"in hP e optimal algoso rt , so H ash join is th ith m ere. Note"
that picking a wrong join type hurts the performance here:
"in the HyPer database system, replacing index-nested loop"
"with hash in 11 results in 50% penalty, and similar e↵ects"
are observed in the Virtuoso RDBMS.
Determining the join type in Query 9 is of course a conse-
"quence of accurate cardinality estimation in a graph, i.e. in a"
"dataset with power-law distribution. In this query, the opti-"
mizer needs to estimate the size of second-degree friendship
circle in a dense social graph.
"Finally, this query opens another opportunity for databases"
"where each stored entity has a unique synthetic identifier,"
"e.g. in RDF or various graph models. There, the system"
en-n stmay cho oo ese mt o a ssi mgn si od nt Pe ifiers to ts C/
tities such that their IDs are increasing in time (creation
"time of the post). Then, the final selection of Posts/Com-"
ments created before a certain date will have high locality.
"Moreover, it will eliminate the need for sorting at the end."
4. SNB-INTERACTIVE WORKLOAD
The SNB-Interactive workload consists of 3 query classes:
Transactional update queries. Insert operations in SNB
are generated by the data generator. Since the structure
"of the SNB dataset is complex, the driver cannot generate"
"new data on-the-fly, rather it is pre-generated. DATAGEN"
"can divide its output in two parts, splitting all data at one"
particular timestamp: all data before this point is output in
"the requested bulk-load format (e.g., CSV), the data with a"
timestamp after the split is formatted as input files for the
query driver. These become inserts that are “played out”
as the transactional update stream. There are the following
types of update queries in the generated data: add a user
"account, add friendship, add a forum to the social network,"
"create forum membership for a user, add a post/comment,"
add a like to a post/comment.
Complex read-only queries. The 14 read-only queries shown
in the Appendix retrieve information about the social envi-
"ronment of a given user (one- or two-hop friendship area),"
"such as new groups that the friends have joined, new hash-"
"tags that the environment has used in recent posts, etc. Al-"
though they answer plausible questions that a user of a real
"social network may need, their complexity is typically be-"
yond the functionality of modern social network providers
"due to their online nature (e.g., no pre-computation). These"
queries present the core of query optimization choke points
in the benchmark. We have already discussed some of the
challenges included in Query 9 in Section 3; the analysis of
the rest of the queries is given in [11]. The base definition of
"the queries is in English, from the LDBC website4 one can"
"find query definitions in SPARQL, Cypher and SQL, as well"
as API reference implementations for Neo4j and Sparksee.
Simple read-only queries. The bulk of the user queries are
simpler and perform lookups: (i) Profile view: for a given
"user returns basic information from her profile (name, city,"
"age), and the list of at most 20 friends and their posts. (ii)"
Post view: for a given post return basic stats (when was it
submitted?) and some information about the sender.
We connect simple with complex read-only queries using
a random walk: results of the latter queries (typically a
small set of users or posts) become input for simple read-
"only queries, where Profile lookup provides an input for Post"
"lookup, and vice versa. This chain of operations is governed"
by two parameters: the probability to pick an element from
"the previous iteration P , and the step with which this"
"probability is decreased at every iteration. Clearly, since"
"the probability to continue lookups decreases at each step,"
the chain will be finite.
Q Cu .e nsry M oi trux cting the overall query mix involves
defining the number of occurrences of each query type. While
"doing so, we have two goals in mind. First, the overall"
"mix has to be somewhat realistic. In a social network, this"
"means the workload is read-dominated: for instance, Face-"
book recently revealed that for each 500 reads there is 1
"write in their social network [15]. Second, the workload has"
"to be challenging for the query engine, and consequently the"
throughput on complex read-only queries should determine
a significant part of the benchmark score.
"and friends of friends, and then going through the forums"
to filter out those that all these friends joined after a cer-
tain date. It is therefore su cient to select parameters with
similar runtime for the given query plan.
"Now, the problem of selecting (curating) parameters from"
the corresponding domain P with properties P1-P3 can be
formalized as follows:
oruP for the Intended Query Plan QIa iCer ata :me nt r
"and the paramPeter domain P , select a subset S ⇢ P of size"
iqout Sp k such that 8 IT Qqi2 Variance8 2 C (T (p)) is mini-
mized. This problem definition requires that the total vari-
"ance of the intermediate results, taken for every subplan T"
qi
"of the plan QI, is minimized across the parameter domain"
P (in case of multiple parameters P is a cross-product of the
respective domains). Since the cost function correlates with
"runtime, queries with identical optimal plans w.r.t. C and"
tou
similar values of the cost function are likely to have close-
to-normal distribution of runtimes with small variance.
"From the computational complexity point of view, the Pa-"
"rameter Curation problem is not trivial. Intuitively, an exact"
algorithm would need to tackle a problem which is inverse
to the NP-hard join ordering problem: for the given optimal
"plan find the parameters (i.e., queries) which yield a given"
"cost function value. Clearly, we can only seek a heuristic"
method to solve this at scale.
"Note that, as opposed to estimates of C (that could be"
o tu
"obtained from an EXPLAIN feature), we use the de facto"
amounts of intermediate result cardinalities (which are oth-
erwise only known after the query is executed).
aP tea aar im at ne .r C Our heuristic for scalaberu o t sc lel
Parameter Curation works in two steps:
Step 1: Preprocessing The goal of this stage is to compute
all the intermediate results in the query plan for each value
of the parameter. We store this information as a Parameter-
"Count (PC) table, where rows correspond to parameter val-"
"ues, and columns to a specific join result sizes."
"As an example, consider LDBC Query 2, which extracts"
20 posts of the given user’s friends ordered by their times-
"tamps, following the intended plan depicted in Figure 6a."
The Parameter-Count table for this query is given in Fig-
"ure 6b, where columns named | 11 | and | 12 | correspond"
to the amount of intermediate results generated by the first
"and second join, respectively. In other words, when executed"
"with %PersonID = 1542, Query 2 generates 60 + 99 = 159"
intermediate result tuples.
There are two ways to obtain the Parameter-Count table
for the entire domain of PersonID in our example:
"(i) we can form multiple Group-By queries around each sub-"
query in the intended query plan. In our example these are
the queries (Person 1 Friend) and
P sre IDon
"((Person 1 Friend) 1 Forum). The result of"
DP r onse I
these queries are first and second column in Parameter-
"Count table, respectively. Or, alternatively"
"(ii) since we are generating the data anyway, we can keep"
the corresponding counts (number of friends per user and
number of posts per user) as a by-product of data genera-
tion. SNB-Interactive uses this strategy: DATAGEN in a
final stage curates parameters based on frequency statistics.
The Parameter-Count table needs to be materialized for
every query template. While it is feasible for discrete pa-
rameters with reasonably small domains (like PersonID in
where some values have the step-function distribution. TPC-
DS circumvents undesired e↵ects by always selecting param-
"eters with the same value of step function (i.e., from the"
"same “step”). However, this trick becomes impossible when"
the distribution is more complex such as a power-law distri-
"bution, and when there are correlations across joins (struc-"
tural correlations).
"In general, in order for the aggregate runtime to be a use-"
"ful measurement of the system’s performance, the selection"
of parameters for a query template should guarantee the
following properties of the resulting queries:
P1: the query runtime has a bounded variance: the aver-
age runtime should correspond to the behavior of the
majority of the queries
P2: the runtime distribution is stable: di↵erent samples of
"(e.g., 10) parameter bindings used in di↵erent query"
streams should result in an identical runtime distribu-
tion across streams
P3: the optimal logical plan (optimal operator order) of the
queries is the same: this ensures that a specific query
template tests the system’s behavior under the well-
"chosen technical di culty (e.g., handling voluminous"
"joins, or proper cardinality estimation for subqueries)"
It might seem that the ambition in SNB to include queries
that are a↵ected by structure/value correlations goes counter
"to P3, because due to such correlation a particular selection"
predicate value might for instance influence a join hit ra-
"tio in the plan, hence the optimal query plan would vary"
"for di↵erent parameter bindings, and picking the right plan"
"would be part of the challenge of the benchmark. Therefore,"
whenever a query contains correlated parameters we identify
query variants that correspond to di↵erent query plans. For
"each query variant, though, we would like to obtain param-"
"eter bindings with very similar characteristics, i.e. we still"
need parameter curation.
There are two considerations taken into account when de-
signing the procedure to pick parameters satisfying prop-
"erties P1-P3. First, there is a strong correlation between"
the runtime of a query and the amount of intermediate re-
"sults produced during the query execution, denoted C"
uo t
"[9]. Second, as we design the benchmark, we have a specific"
"(intended) query plan for each query. For example, LDBC"
Query 5 mentioned above has an intended query plan as
given in Figure 6a. It should be executed by first looking up
"the person with a given PersonId, then finding her friends"
"IGT is to make GDS composable. That is, a GDS instance"
could track other GDS instances in the same manner as it
"tracks LDS instances, enabling dependency tracking in a"
hierarchical/distributed setting.
"Every operation, regardless ofS it er d sxe na om .MeE c out"
"dependencies, is executed in a similar manner, illustrated in"
Figure 8. The default Execution Mode (method of schedul-
ing operations) is Parallel: multiple stream operations are
"executed in parallel, using a thread pool, and the dependen-"
"GCcies are satisfied using T communication. However, for"
some types of operations it is possible to use a simple Se-
"quential execution mode, where very limited communication"
"between driver threads is necessary, since most of the depen-"
dencies stay within one stream. In this section we describe
the motivation and applicability of this Sequential mode to
update execution in SNB-Interactive.
Some dependencies are di cult to capture e ciently with
"GCT alone. For example, consider a subset of the SNB-"
"Interactive workload: the creation of users, posts, and likes."
"Likes depend on the existence of posts, posts and likes de-"
pend on the existence of the users that created them. Users
"are created at a much lower frequency than posts and likes,"
"and do not immediately create content. Conversely, posts"
are replied to and/or liked soon after their creation.
CGUsing T to maintain dependencies between posts and
"GClikes would result in many frequent updates to T , and"
excessive synchronization between streams as they wait for
"CGT to advance. However, observe that posts and likes"
"form a tree, rooted at the forum, therefore it is possible to"
"partition update streams by forum, eliminating inter-forum"
dependencies. The insight here is that posts and likes only
"depend on other posts from the same forum, as long as intra-"
"forum dependencies are maintained, updates to a given fo-"
rum can progress irrespective of the state of other forums.
"Moreover, when dependent operations occur at high fre-"
EDUquency (duration between T of dependent operations is
short) the benefit of parallel execution might be negated by
the cost of dependency tracking in the query driver. The
alternative o↵ered by Sequential execution is that instead
"of classifying stream operations as Dependent/Dependency,"
the same dependencies can be captured by executing that
"stream sequentially, thereby guaranteeing causal order is"
"maintained. This, however, only applies when it is possible"
"to partition streams into many smaller streams, to achieve"
su cient parallelism.
In the SNB-Interactive case Sequential execution is used
GCfor capturing intra-forum dependencies - using T would
introduce false dependencies. This dramatically reduces over-
"head related to dependency tracking, and achieves su cient"
parallelism due to the large number of forums.
For dependencies between users and their generated con-
"GCtent T tracking is used, as it is impossible to partition the"
social graph in such a way that dependencies are eliminated.
tioW Ei unncd The mechanisms we introducedo xw e d .e
so far guarantee that dependency constraints are not vio-
"lated, but in doing so they unavoidably introduce overhead"
of synchronization between driver threads.
"In so-called Windowed Execution mode, operations are ex-"
"ecuted in groups (Windows), where operations are grouped"
"UEDaccording to their T . Every Window has a Start Time,"
"Duration, and End Time. Logically, all operations in a Win-"
"dow are executed at the same time, some time within the"
"Window. No guaranty is made regarding exactly when, or"
"in what order, an operation will execute within its Window."
Operations belonging to Dependencies are never executed in
D EUthis manner – T of Dependencies operations are never
modified as it would a↵ect how dependencies are tracked.
To allow Windowed Execution mode we must ensure a
DEP UEDminimum duration exists between the T and T of
"any operation in Dependents, this is called “Safe Time”"
"AFES(T ). In the case of the SNB dataset, what we need"
to know is the minimum time between a person becoming a
"member of the network and making a first post, and a min-"
imum time between becoming a friend and writing a first
comment or like in the friend’s forum. DATAGEN ensures
FESAthat this T is considerably long in all generated data.
CGThe end e↵ect of Windowed Execution is that the T be-
tween the parallel threads (or processes) in the driver need to
AS EFbe synchronized much less often (once every T of simu-
"lated time). This helps reduce communication overhead, and"
this mode also gives threads the ability to schedule queries
inside a window out-of-order and hence be less bursty.
"In the currently available version of the driver, which is"
"multi-threaded but single-node, Windowed Execution mode"
is not yet available. We plan to make this available in the
"multi-node version of the driver, where synchronization cost"
would be high (as it involves network communication).
tS epc onla Ed na .bl xe tD nicuee To illustrate driver scal-e
"ability, experiments were performed using a dummy database"
"connector that, rather than executing transactions against"
"a database, simply sleeps for a configured duration. From"
the driver perspective this simulates a benchmark run where
"the SUT takes, on average, that duration to execute a trans-"
action. The experiment was run with two configured sleep
durations: 1ms and 100us.
The chosen workload consists only of the SNB-Interactive
"updates. Specifically, all updates from SF10 update stream"
7 QQ 216 QQ 9Q5 1Q 14 1Q 33 1Q 12 QQ QQ Q1complex read-only,8,,,4Q 0,,,,,,,,,,
"Sparksee,SF10 20 44 441 31 100 41 11",38,,3376,194,66,,,177,,,794,,,2009
"Virtuoso,SF300 941 1493 4232 1163 2688 16090 1000",32,,18464,1257,762,,,1519,,,559,,,742
"- approximately 32 Million operations. Note that, as they"
"contain no inter-dependencies, executing the read queries in"
parallel is trivial and uninteresting from the perspective of
"driver scalability. To control parallelism, the number of par-"
titions was set from 1 to 12. All experiments were done on
"a system with 12 Intel Xeon E5-2640 CPUs, 128GB RAM,"
and SSD storage running Linux’s 3.2.0-57 kernel.
"As presented in Table 5, the driver shows near-linear scal-"
ability while maintaining the complex inter-partition depen-
"dencies, i.e., ensuring dependent operations do not start"
until their dependencies complete. Every entry in Table 5
corresponds to the execution of the exact same stream (in
"SF10 the stream comprises of 32,648,010 forum operations"
"and 6,889 user operations, which are spread uniformly across"
"the timeline), the only di↵erences being the number of par-"
titions the stream is divided into. As partition count (par-
"allelism) increases so does the execution rate, reducing the"
"time between subsequent operations, in turn straining more"
the tracking of dependencies. Remembering that every fo-
"rum operation depends on one user operation, this increased"
throughput translates to a greater probability for any of the
forum operations to block as they wait for their user depen-
"dency to complete. Further, because forums are executed"
"using synchronous execution mode, the blocking of one op-"
eration would result in the blocking of an entire partition.
5. EVALUATION
We report some results of running SNB-Interactive with
two systems: Sparksee (native graph database) and Virtu-
oso (hybrid relational/RDF store). All the runs were per-
"formed on the same machine, a single dual Xeon E5 2630"
with 192GB of RAM and 6 magnetic disks. It should be
noted that both vendors are still in the process of tuning
their systems and these results are preliminary.
0S Sc apoalFirst we run the Interactiveae F c rt kr ees:1
workload on SF10 dataset with Sparksee graph database.
The queries were implemented using Sparksee’s Java API.
"Sparksee creates indexes on IDs of nodes, and additionally"
materializes neighborhood of each node. Our acceleration
"ratio for this run is 0.1, measured throughput is around 26"
"operations per second. For the 10GB dataset, the Sparksee"
image takes 27GB; the execution is therefore fully in mem-
ory. We provide the mean latencies of complex read queries
in Table 6. Results of short read queries and transactional
"updates are given in Tables 7 and 9, respectively."
We run the SNB SF300 onaeS 3ac ool urF otc i00 : sV rt
Openlink Virtuoso 7.5. The benchmark implementation is
in SQL using Virtuoso transitive SQL extensions for graph
traversals. The tables are column-wise compressed and in-
"dices are created on foreign key columns where needed, oth-"
erwise all is in primary key order. The 300GB dataset is
88GB after gzip compression.
In Table 8 we give the sizes in MB of allocated database
"pages for three largest tables and their largest indices, loaded"
into Virtuoso Column store. 138GB is the total allocated
space including all column and row-wise structures. We note
that both identifiers and datetimes compress significantly
from their CSV form.
The benchmark was run with acceleration of 10 units of
"simulation time per 4 of real time (0.4), reaching a through-"
put of 500 queries per second. Table 6 gives the number
of executions and mean client-side duration of each query.
The achieved throughput would be plausible as a peak load
"of an online system, as these are usually sized to run at"
"a fraction of theoretical peak throughput. Thus, a 300GB"
dataset with 1.1 million people could be served from a smal-
"l/medium commodity server (12 core, 192GB RAM). Con-"
"versely, a 100 million people network would take over 200"
"servers in a redundant cluster configuration, which is again"
plausible. Tables 7 and 9 have results of short read queries
and transactional updates of the Virtuoso SF-300 run. More
experimental results for Virtuoso can be found in [12].
6. RELATED WORK
Benchmarking has become popular in the RDF/Seman-
"tic Web community, partially because there is a standard"
"query language, SPARQL. Consequently many benchmarks"
"exist in the area, including LUBM, BSBM, and SP 2Bench."
Although these benchmarks often cover many features of
"SPARQL, even BSBM, a rather advanced benchmark, does"
not reach the classical TPC-H in terms of query optimization
"challenges. SNB-Interactive workload, on the other hand,"
"follows a query language-independent, choke point-based ap-"
proach to provide challenges for modern DBMS.
"Related to social network benchmarking, Facebook re-"
"cently presented Linkbench [1], a benchmark targetting the"
"OLTP workload on the Facebook graph. It is, however,"
rather limited in scope (only transactions) and uses a syn-
"thetic graph generator that, besides degree distribution, re-"
produces very little of the structure or value correlations
"found in real networks. In contrast, SNB’s DATAGEN pro-"
vides the ground for multiple realistic workloads ranging
from OLTP to graph algorithms.
The BG benchmark [2] proposes to evaluate simple so-
cial networking actions under di↵erent Service Level Agree-
ments. We note that LDBC queries are more complex than
"BG, and require more stict consistency requirements (ACID)."
"Further, it would be impossible to validate benchmark runs"
for such relaxed consistency models.
"In the super-computing domain, we find Graph-500, which"
"consists of Breadth First Search queries, and is used to test"
the hardware capabilities of large scale systems for work-
T lab,Mee,sgS ti )ze ( BM,,)L ea r,,(d xI Bn,,
post,,76815,,ps content,,,(41697),
likes,,23645,,l creationdate,,,(11308),
forum person,,9343,,fp creationdate,,,,(5957)
APPENDIX,,,,,,,,,,,,,,,,
dsnDefi 1n oitiSo f ue1 r4 r iead iq ees,ao ]f vtt rh oe ltI n,e,a,c,,,1[oe,ew r k,,,,,.,,,
wf So R/r Pt Qh yralrei AQ LL /S tirC my,fp rh e euo hau,,,,o,qn,s,e. eF o,,,c,,,r,,
rta add ai noi t ra l ml .y hh i tgh eli e sg ti s p,,,,,,,,,,,,,,,,
Q1. iE ierx tht rr fca dc t d ne Gs i ep t vi emn ni ao,aof vn s,new,,,,,,g i,,,,,,,,
"itsa rp 02re ,r eto n’ os fi rs tNa me thewr he emt eu",n u p fio,p,,,spl,,,,,,,,s ta,,,
")n p rsa dm , afe oos r nte vd iyb ei nxrc ane ra 3is nn g",i es gmt c ( om,",a",,,,,,,,,,e,,,,
ma in eed af nio sr hp te lo hep w n dt,asri descat,.men,,,,os,,te y lb,,,,,,t,a,
yR s re .ccnu idltl us dus dph tdlkpo u cil oeeet ah we,l ts o f,,,oa,,s,a,sfn,l,,,,s,,,
Q2. yewenFi nehd tos st r2 um0 p,no ts t s ea n id rc .o,sdnfrm,,m,e,,,f o,,,,,,,,
"mG ai ov oe fin ea ds Pt ,a tr rso rn n )d",s( ns mt se sc,ee on,,nt,,tP,,Ct,,,,,,m,,
"ef tr ro dm oa ,l fl ro ef ast neh na at ’sP dte tr brs eo in f",h da,w,,e,(r,,ec,e,,,,,,,,
"opue C. R ani en tsluc nd ngi ) mia g dv e ,Date",nt r 0t /h oe t,,2,,,sP,,ts,no,,,m,,,,
aet ch tse hP r e teo tn a r yed a esc,innh mh to f t e g b. So,,,ltur,,r,,s,dd,cs ee,,,,,,
"cc onr d nte t ta i io edn ad te, a sn h e .n fia rs",e gd yi en b,itP,,,,,,,,,,,,,,
Q3. oceF vhr Xsdi tht in tw ei n 2 os,lp s yt ta irr tee se,cn t,,,,,ra,ul ee d,nt,,,,,,,
enipFa fn dd iiY 2. i sn ad rft ndo fi0 nr oe grn vdsd,,f,,of,,ae en s,,,,,,,,Person,,
rvw eoh ynah om d ea te oa inp to fhos ntntr,a c Com gm ue,,,,,,,Xe,ri,,,,,,,
DurationInDaysrs ewa in pd CountryY ii ot fh fii en a,c dd p e o,,,,,,,,,,,,,,rteaf,
rDa entst oa lr t ea bm et de .s. rS fortet ne bs tuu,dd nsc i sg,,,py,,o,ot,a sl,,,,,,,,
Q4. .w TN opi se c hG ei tv ne on,"Perssa t ,a 1r dt",,,n,,,fi,,,,t p,so to,,m0,,
bp oybo pp fnu msal r T tag ( h)serto tat hl u,eho ts,,twi,,,,,a -ta,tg,,,eart,a,,,
tdt nda wsoc h d ie t no P sr’st it a tth tt harw nye er e sc,rsoe a e b,,,,f iP,,,,,,he,,,,,
timga vali .nev e inter,,,,,,,,,,,,,,,,
",n G.s ur mvi eor up og tw fitN sQ5. Fre",hsa t a osr enP te,o,n,,,,2d,,,,p,,0,,,
eat nh Pe nsoienf tr i ne dd s ra ed af jr desso oif,rf i dn arf eno f,t h,,,,,,,,,,,,,t,
umlg tei sv nhe nn Date. doS rt eer su fnndst,e c issi rg tb,by,,,,e,,,,,,Poo,,,,
te a oc s.nh eFo rhr tu sm ot eah srew e dc eatree,Pb y fa n y,,,,,,,,,,,,,,,
"Q6. cT ra g ro rev-o Gc ic u sr dse ,n c emPe .",nnn a t,aa,,t,,e,,o,,,,,T gs ao,,,
wofin gtd hht oce ao ot nh r T stehges stita oPs it t,cu r h,er,,,,,th,,agT,,,,,,,
at dh eiPea st os nw e e nr d’ec rr Re-dye t d nrb,na ds rf e is,,,,,f,,eo,irff,,,,n,.s,,
"tst tu sr on nt o f,p o01 T ugsa ces bgor ht ie d d",tces tae n od,y,,nt,h,,,,,,,P,,,,
nw nhie tr oce Tc sr obe snea htht ie dd sy gt th e oP ae sr,"a, aw ic nh",,,,,,,b,,,,,,,,
".eth Tg ai ev n g",,,,,,,,,,,,,,,,
Q7. rlR me tc ne sn ot ii gk e os e. sF r tt nch,e tes ep te efid,erP,,,,,,ch,oe,,,,,,,
el k ht ei thfafoe pes y onar ets to wan te’,"es lp no cns s y,",d,t,bh,e,,,en,,,,,,,,
udc kno etr eetr de nas soop oi n eig p id h,oe l ik m. lF,fa tg,,tL,,,,hs sr,,,,,,,,
ed rci dr Lec et o . Recnn ikt ri do un tssctn,po 2 i0,g bye,nos,"d,",,enr,,e de,,,,,,,,
foc her itea eki nd ta .e lt o,,,,,,,,,,,,,,,,
Q8. eoM es eet m0her tpe iece vrn et lyi,rs s. rT uh i q,,,,,r,,tos,,,s2t,,,,,
"lr eee ncetpon srt ,ofp l tny lc mmo e t os et a",h ms st,Personan,,d c,,,mo,,,,,,,,,
do i ndr eae osr e g bd ad e c re ynndi tc.e,t,,,,,,,,,,,,,,,
Q9. tsaLet se ms dt P nonho ct ms. F i atnd t,e nm ceo ts r,,,et,2,sos0 p,,,,,,,,,,
"sf cr beom ea el -l df r ni fe in bd es e, oo rr tf sr ri fe tan dod f-","uo ,f",Person,,,,,,,,r,,,,,,
"e.Da ea it oR nre aart su er ,n drsp no rs ct t tah","ti se ,sat o er",,,,td,,oc,,,,,,,,,
did tne .es yc e d n aae t og bicr n,,,,,,,,,,,,,,,,
Q10. rF aer edi de nn er omc sdm tn nfrd,.a it fi o 0n iFn,,,o,dp,,i1,e,,,,,o f,,,
otphow ntieo us et tas am ldu foch b u htt,rn ae s et ltr os o,,,Pe s,,,,in,,,,,,,b,
en tno aut ii rete rr fss tet ogni bit pc stro ch,es r. T rh ee,his,,,,,h,s e,,,,ec,ydt,,,
whdc oroscopea ’n td ei a s he dS -ig fon,r. tR ne ↵tu rrn fs,oir e,,,d,s,,h,,,m,,,,ei,
e in rc eoe swb te ht shte e sfn rme tt uo tt tta tl ibn eou,se r po he,,,,,,,na b,,,,,e,,,
eo ef rt ifih ts aue ps ae ec irod uu s ohtdn,sth e tt bo a t pbel n,m,,,r,,,f,,,,,,,,o
rt sno sep ii eic os t geeh iaa t ba e es oft .n ut osr,"lt,t lh s",er,,,,,s pra,ssa,,,,,,,,
nS ↵bsuo yr st tt eh t iree ciel d sc n eed ehi rn .g,d,,,,,,,,,,,,,,,
"Q11. PersonJ rf0bo hir ie ectf de opr ,near il f. F n pstd",o 1,fid,s,,,,,e e,,,,,,,,
"eor a f eir de hn pd ocef eh ine asr f sonr eei ,) wd h( sx",luc d ri pn g fit,,,,,,,,,,,,,,ho,
dl pegon ewo trk sde cai tcn aa eo nm C up rony i,Sn rc fii,,,go,,,n,y i.,,,,,da ns,,,
"b ny hs ret a tnt, ar at natd e sn d idiic",.re pnned g fieby,,,os,,,,e,,,,,,,,
Q12. dE odsniexp rSe ar ft ne rPFa er oc iih v.,f,wa,,,,er,s,dn eh,lo,,h,,,e,p,
httthe im vo pws ot st o p s a net.a,Rng ou ni ea gi,TagCategory,,,,,,,,,,,tt,r,,
"p02 ie ybeder ns oro lbn es spee, is mo rfr et nd .us dcn",g,,,,,,,,,,,,,,,
Q13. etSi fitn ng s nXl ae as ostrho p,"dh i e. o Y ,G nnv dn Per",,,,,,,Pers,,,,,,,,
it she ehh tor e est np tudna abgth hb ue ht e sw bydee n,t im t,,,,,,r,p,,,,hec,,,,
onK ao htw ls r he uteRl ii s.a htt hpi to ns h s ptnr,.fe e gn,,,,,,,,,,,,,,,
"Q14. G.W ane i lg ih ret le vd aPeap t , finsh",n son,,,X,,,dd,Pe rsonY,,,,,,,,
hw lhei tg emht ese rad op -to hnf it he s,ns et ug h bt,,het,,e n tw e,,,,,,,,st,be,,
ng lr sa fi hgtp hh i thodn atu oc te d eyb oKt h ense pw,re pa i hi,,,.,hT,,,ew,,,,,,,,
".nt dea ek ts i t on fo asc sut ono i emd se C/r Poa sim",to xn,,,,,o,nm,ne,ht,,,,c,ga,,
7. CONCLUSION,
The LDBC SNB introduces a new and quite complex syn-,
thetic social network dataset on which three workloads are,
"intended to be run: SNB-Interactive, SNB-BI and SNB Al-",
"gorithms. This paper focuses on the former, which tests",
on-line queries. The benchmark has been implemented on,
"graph database systems, RDF database systems and RDBMSs.",
The SNB data generator is innovative due to its power-,
law driven data generation with realistic correlations be-,
"tween properties and graph structure, as well as its scalable",
Hadoop implementation – allowing to generate terabytes of,
data quickly on a small cluster.,The SNB query driver
needed to confront the issue of non-partitionability of the,
"transaction workload, since social graphs are one huge con-",
nected component. The SNB-Interactive query mix is a bal-,
ance between testing so-called “choke points” with the com-,
"plex read-only queries, and executing simple updates and",
read-only queries. A final contribution is the introduction,
of parameter curation which data mines the dataset for query,
"parameters with highly similar behavior, to make the bench-",
mark score more insightful and stable across runs.,
"SNB-Interactive contains a rich set of technical challenges,",
of which we are convinced that the current generation of,
systems only a few target e↵ectively. This makes it an in-,
"teresting benchmark for IT practitioners, industry engineers",
and academics alike.,
